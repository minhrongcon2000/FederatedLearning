{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96173acc-b25e-4464-ae76-441ab800e8aa",
   "metadata": {},
   "source": [
    "# Coding Assignment \"Bonus #1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a9af84-405e-48b1-ab8d-f4eb43d2b4ba",
   "metadata": {},
   "source": [
    "__Goal:__ Make accurate predictions on the validation nodes.\n",
    "<br>\n",
    "__Task description:__ Ten weather stations have been randomly chosen as validation nodes (see plot in Section 2.3). Therefore, they __do not participate in the training__. You can imagine, that they lost their temperature measurements (labels). You must estimate the models' parameters of the validation nodes using any method(s) covered by Lecture Notes (see link below) and calculate the MSE for each validation node. Finally, calculate the average of all MSEs of the validation nodes.\n",
    "<br>\n",
    "__Important:__ do not modify the sections 1.2, 2.1, 2.2, and 2.3!\n",
    "<br>\n",
    "__Grading:__ the submissions will be rated based on the achieved average error over validation nodes. The submission with the minimum error will be graded with 11 points, the second best result will be graded with 10 points, ... the fifth best result will be graded with 7 points. All other submissions will be graded with 6 points (if the achieved average error is __less than 30__) or 0 points (if the achieved average error is __greater than or equal to 30__). \n",
    "<br>\n",
    "__Hints:__\n",
    "* Section 1.2 contains the _val_nodes_avg_error()_ function that calculates the average of MSEs of the validation nodes. \n",
    "* Try to construct relevant edges between similar nodes.\n",
    "* Try to work with clustered data set.\n",
    "* Usefull algorithms: FedGD, FedAvg, KMeans. \n",
    "* Try out different hyperparameters.\n",
    "* You can copy code from previous assignments including the reference solutions.\n",
    "\n",
    "Lecture Notes: https://github.com/alexjungaalto/FederatedLearning/blob/main/material/FL_LectureNotes.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9d97d3-1648-4d9b-8b36-5573b1f9fd82",
   "metadata": {},
   "source": [
    "## 1. Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03a531a-4a66-4f02-9534-2ab2377e81ef",
   "metadata": {},
   "source": [
    "### 1.1. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46594b15-a21e-4ec9-8860-460b3c0c4b1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Modules.\n",
    "import colorsys\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import networkx as nx \n",
    "\n",
    "# Submodules\n",
    "from numpy import linalg as LA\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# Methods\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cd6231-749f-4963-aa44-afcc64d9cfbb",
   "metadata": {},
   "source": [
    "### 1.2. Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e1add17-a9be-4e89-9144-77445218abbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The function generates returns the numpy array\n",
    "# of num_colors distinctive colors in RGB format.\n",
    "def generate_distinctive_colors(num_colors):\n",
    "    colors = []\n",
    "    hue_step = 1.0 / num_colors\n",
    "    saturation = 0.7\n",
    "    value = 0.9\n",
    "    for i in range(num_colors):\n",
    "        hue = i * hue_step\n",
    "        rgb = colorsys.hsv_to_rgb(hue, saturation, value)\n",
    "        colors.append(rgb)\n",
    "    return np.array(colors)\n",
    "\n",
    "# The function generates a scatter plot of nodes (=FMI stations) using \n",
    "# latitude and longitude as coordinates. \n",
    "def plotFMI(G_FMI, highlight_val_nodes=True):\n",
    "    # Get the number of clusters.\n",
    "    num_clusters = len(set([G_FMI.nodes[node]['cluster'] for node in G_FMI.nodes]))\n",
    "    \n",
    "    # Get the colors for clusters (+1 for validation nodes).\n",
    "    colors = generate_distinctive_colors(num_clusters + 1)\n",
    "    \n",
    "    # Get the coordinates of the stations.\n",
    "    coords = np.array([G_FMI.nodes[node]['coord'] for node in G_FMI.nodes])\n",
    "    \n",
    "    # Draw nodes\n",
    "    for node in G_FMI.nodes:\n",
    "        if G_FMI.nodes[node]['validation'] and highlight_val_nodes:\n",
    "            color = colors[-1]\n",
    "            plt.scatter(coords[node,1], coords[node,0], color=color, s=4, zorder=5)  # zorder ensures nodes are on top of edges\n",
    "            plt.text(coords[node,1]+0.1, coords[node,0]+0.2, f\"Validation ({node})\", fontsize=8, ha='center', va='center', color=color, fontweight='bold')\n",
    "        else:\n",
    "            color = colors[G_FMI.nodes[node]['cluster']]\n",
    "            plt.scatter(coords[node,1], coords[node,0], color=color, s=4, zorder=5)  # zorder ensures nodes are on top of edges\n",
    "            plt.text(coords[node,1]+0.1, coords[node,0]+0.2, str(node), fontsize=8, ha='center', va='center', color=color, fontweight='bold')\n",
    "    # Draw edges\n",
    "    for edge in G_FMI.edges:\n",
    "        plt.plot([coords[edge[0],1],coords[edge[1],1]], [coords[edge[0],0],coords[edge[1],0]], linestyle='-', color='gray', alpha=0.5)\n",
    "\n",
    "    plt.xlabel('longitude')\n",
    "    plt.ylabel('latitude')\n",
    "    plt.title('FMI stations')\n",
    "    plt.show()\n",
    "\n",
    "# The function below extracts a feature and label from each row \n",
    "# of dataframe df. Each row is expected to hold a FMI weather \n",
    "# measurement with cols \"Latitude\", \"Longitude\", \"temp\", \"Timestamp\". \n",
    "# Returns numpy arrays X, y.\n",
    "def ExtractFeatureMatrixLabelVector(data):\n",
    "    n_features = 7 \n",
    "    n_datapoints = len(data)\n",
    "    \n",
    "    # We build the feature matrix X (each of its rows hold the features of a data point) \n",
    "    # and the label vector y (whose entries hold the labels of data points).\n",
    "    X = np.zeros((n_datapoints, n_features))\n",
    "    y = np.zeros((n_datapoints, 1))\n",
    "\n",
    "    # Iterate over all rows in dataframe and create corresponding feature vector and label. \n",
    "    for i in range(n_datapoints):\n",
    "        # Latitude of FMI station, normalized by 100. \n",
    "        lat = float(data['Latitude'].iloc[i])/100\n",
    "        # Longitude of FMI station, normalized by 100.\n",
    "        lon = float(data['Longitude'].iloc[i])/100\n",
    "        # Temperature value of the data point.\n",
    "        tmp = data['temp'].iloc[i]\n",
    "        # Read the date and time of the temperature measurement. \n",
    "        date_object = datetime.strptime(data['Timestamp'].iloc[i], '%Y-%m-%d %H:%M:%S')\n",
    "        # Extract year, month, day, hour, and minute. Normalize these values \n",
    "        # to ensure that the features are in range [0,1].\n",
    "        year = float(date_object.year)/2025\n",
    "        month = float(date_object.month)/13\n",
    "        day = float(date_object.day)/32\n",
    "        hour = float(date_object.hour)/25\n",
    "        minute = float(date_object.minute)/61\n",
    "        \n",
    "        # Store the data point's features and a label.\n",
    "        X[i,:] = [lat, lon, year, month, day, hour, minute]\n",
    "        y[i,:] = tmp\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def choose_val_nodes(graph_FMI, n_nodes, seed=4740):\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Copy the nodes to a new graph.\n",
    "    graph = graph_FMI.copy()\n",
    "    \n",
    "    # Choose validation nodes.\n",
    "    val_nodes = np.random.choice(graph.nodes, size=n_nodes)\n",
    "    for val_node in val_nodes:\n",
    "        graph.nodes[val_node]['validation'] = True\n",
    "    \n",
    "    return graph\n",
    "\n",
    "def get_train_val_nodes(graph_FMI):\n",
    "    # Copy the nodes to a new graph.\n",
    "    graph = graph_FMI.copy()\n",
    "    \n",
    "    # Create storages.\n",
    "    train_nodes = []\n",
    "    val_nodes = []\n",
    "    \n",
    "    # Distribute the nodes.\n",
    "    for node in graph.nodes:\n",
    "        if graph.nodes[node]['validation']:\n",
    "            val_nodes.append(node)\n",
    "        else:\n",
    "            train_nodes.append(node)\n",
    "        \n",
    "    # Make numpy arrays.\n",
    "    train_nodes = np.array(train_nodes)\n",
    "    val_nodes = np.array(val_nodes)\n",
    "    \n",
    "    return train_nodes, val_nodes\n",
    "\n",
    "def val_nodes_avg_error(graph_FMI):\n",
    "    # Copy the nodes to a new graph.\n",
    "    graph = graph_FMI.copy()\n",
    "    \n",
    "    # Get validation nodes.\n",
    "    _, val_nodes = get_train_val_nodes(graph)\n",
    "    \n",
    "    # Create storage for the validation errors.\n",
    "    val_errors = np.zeros(len(val_nodes))\n",
    "    \n",
    "    for i, val_node in enumerate(val_nodes):\n",
    "        # Calculate the errors of validation nodes\n",
    "        X_val_node = graph.nodes[val_node]['X']\n",
    "        y_val_node = graph.nodes[val_node]['y']\n",
    "        w_val_node = graph.nodes[val_node]['weights']\n",
    "        val_errors[i] = mean_squared_error(y_val_node, X_val_node.dot(w_val_node))\n",
    "        \n",
    "    return np.mean(val_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18ddb32-c4a9-4c25-b466-27825db9bb5c",
   "metadata": {},
   "source": [
    "## 2. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2914fca0-5ab7-44e1-9aef-d8036f6c794a",
   "metadata": {},
   "source": [
    "### 2.1. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53815e9d-f652-4f79-a8b4-9fd3ebd56a25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the weather measurements.\n",
    "data_FMI = pd.read_csv('Assignment_MLBasicsData.csv')\n",
    "\n",
    "# We consider each temperature measurement (=a row in dataframe) as a \n",
    "# separate data point.\n",
    "# Get the numbers of data points and the unique stations.\n",
    "n_stations = len(data_FMI.name.unique())\n",
    "n_datapoints = len(data_FMI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd043995-df1d-4453-bbd5-1027be9adc90",
   "metadata": {},
   "source": [
    "### 2.2. Features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6225a18d-acbb-440d-a88b-2c0bca6b571f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The created feature matrix contains 19768 entries of 7 features each.\n",
      "The created label vector contains 19768 measurements.\n"
     ]
    }
   ],
   "source": [
    "# Extract features and labels from the FMI data.\n",
    "X, y = ExtractFeatureMatrixLabelVector(data_FMI)\n",
    "\n",
    "print(f\"The created feature matrix contains {np.shape(X)[0]} entries of {np.shape(X)[1]} features each.\")\n",
    "print(f\"The created label vector contains {np.shape(y)[0]} measurements.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9ed241-5d8d-4e97-b59b-6565e09d7225",
   "metadata": {},
   "source": [
    "### 2.3. Empirical graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1fcae0b-f8db-4def-aa7b-c75a9d48786e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a networkX graph\n",
    "G_FMI = nx.Graph()\n",
    "\n",
    "# Add a one node per station\n",
    "G_FMI.add_nodes_from(range(0, n_stations))\n",
    "\n",
    "for i, station in enumerate(data_FMI.name.unique()):\n",
    "    # Extract data of a certain station\n",
    "    station_data = data_FMI[data_FMI.name==station]\n",
    "    \n",
    "    # Extract features and labels of a certain station.\n",
    "    X_node, y_node = ExtractFeatureMatrixLabelVector(station_data)\n",
    "    \n",
    "    # Store the station's data in the node's attributes. \n",
    "    G_FMI.nodes[i]['samplesize'] = len(y_node) # The number of measurements of the i-th weather station.\n",
    "    G_FMI.nodes[i]['name'] = station # The name of the i-th weather station.\n",
    "    G_FMI.nodes[i]['coord'] = np.array([station_data.Latitude.iloc[0], station_data.Longitude.iloc[0]]) # The coordinates of the i-th weather station.\n",
    "    G_FMI.nodes[i]['X'] = X_node # The feature matrix for local dataset at node i.\n",
    "    G_FMI.nodes[i]['y'] = y_node  # The  label vector for local dataset at node i.\n",
    "    G_FMI.nodes[i]['z'] = None # The representation vector for local dataset at node i.\n",
    "    G_FMI.nodes[i]['weights'] = np.zeros((X_node.shape[1], 1)) # The weight vector of the local model at node i.\n",
    "    G_FMI.nodes[i]['cluster'] = 0 # The cluster to which the node is assigned (default value = 0).\n",
    "    G_FMI.nodes[i]['validation'] = False # If the node is validation or not.\n",
    "\n",
    "# Choose validation nodes.\n",
    "G_FMI = choose_val_nodes(G_FMI, 10, seed=4740)\n",
    "\n",
    "# Visualize the empirical graph.\n",
    "plotFMI(G_FMI, highlight_val_nodes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f0352f-e67f-40db-b2b6-8bd7db4ebb2f",
   "metadata": {},
   "source": [
    "## 3. Student task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8acc5b2d-0239-49a8-bb60-494a5b872210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1, average loss: 163.7382470121706, val loss: 153.5957746547707\n",
      "Iteration: 2, average loss: 155.74281460536275, val loss: 145.60944445095012\n",
      "Iteration: 3, average loss: 147.9973622865723, val loss: 137.29590251911242\n",
      "Iteration: 4, average loss: 139.92728352892226, val loss: 128.29180487541217\n",
      "Iteration: 5, average loss: 131.17748594053703, val loss: 118.46586311284143\n",
      "Iteration: 6, average loss: 121.6147593760836, val loss: 107.9115688748416\n",
      "Iteration: 7, average loss: 111.3238155392633, val loss: 97.02013091839233\n",
      "Iteration: 8, average loss: 100.67655384598147, val loss: 86.38154066144774\n",
      "Iteration: 9, average loss: 90.23929533290402, val loss: 76.69428314142081\n",
      "Iteration: 10, average loss: 80.68645179674822, val loss: 68.56640727323565\n",
      "Iteration: 11, average loss: 72.61290089980416, val loss: 62.34304168058725\n",
      "Iteration: 12, average loss: 66.36566790060145, val loss: 58.01283850038917\n",
      "Iteration: 13, average loss: 61.951431366556506, val loss: 55.266063446061786\n",
      "Iteration: 14, average loss: 59.08759166883386, val loss: 53.65949359128776\n",
      "Iteration: 15, average loss: 57.3570952254217, val loss: 52.77248277819723\n",
      "Iteration: 16, average loss: 56.357683338404854, val loss: 52.290724346609935\n",
      "Iteration: 17, average loss: 55.78387950583932, val loss: 52.01573774182653\n",
      "Iteration: 18, average loss: 55.438736646071725, val loss: 51.83677634375211\n",
      "Iteration: 19, average loss: 55.209085333174556, val loss: 51.698090298695774\n",
      "Iteration: 20, average loss: 55.03523359667276, val loss: 51.57405647633489\n",
      "Iteration: 21, average loss: 54.88760711375066, val loss: 51.45395957690225\n",
      "Iteration: 22, average loss: 54.75218591137209, val loss: 51.33388092073646\n",
      "Iteration: 23, average loss: 54.62253649799144, val loss: 51.21275941995058\n",
      "Iteration: 24, average loss: 54.49577448333519, val loss: 51.090604364442605\n",
      "Iteration: 25, average loss: 54.37064199862273, val loss: 50.9677550138514\n",
      "Iteration: 26, average loss: 54.24659027108824, val loss: 50.8445659752134\n",
      "Iteration: 27, average loss: 54.12339034287826, val loss: 50.72135694852065\n",
      "Iteration: 28, average loss: 54.00095920516673, val loss: 50.59837294137442\n",
      "Iteration: 29, average loss: 53.87928058099055, val loss: 50.47575732664923\n",
      "Iteration: 30, average loss: 53.7583033013459, val loss: 50.35356519765562\n",
      "Iteration: 31, average loss: 53.63797206924733, val loss: 50.2318538723726\n",
      "Iteration: 32, average loss: 53.51827348718321, val loss: 50.11064148443278\n",
      "Iteration: 33, average loss: 53.39917271148755, val loss: 49.98998077384718\n",
      "Iteration: 34, average loss: 53.280674194944076, val loss: 49.86994517278602\n",
      "Iteration: 35, average loss: 53.1628224193186, val loss: 49.750436496490835\n",
      "Iteration: 36, average loss: 53.04553624858027, val loss: 49.63142461479822\n",
      "Iteration: 37, average loss: 52.928769678309344, val loss: 49.512890328188064\n",
      "Iteration: 38, average loss: 52.81249864550605, val loss: 49.394813936681615\n",
      "Iteration: 39, average loss: 52.696704705556236, val loss: 49.27717629043885\n",
      "Iteration: 40, average loss: 52.58136428842222, val loss: 49.159972050341544\n",
      "Iteration: 41, average loss: 52.46647117909602, val loss: 49.04319854490074\n",
      "Iteration: 42, average loss: 52.352014136199216, val loss: 48.926832696715806\n",
      "Iteration: 43, average loss: 52.23797909649098, val loss: 48.81088557278447\n",
      "Iteration: 44, average loss: 52.12435864480797, val loss: 48.69537657568297\n",
      "Iteration: 45, average loss: 52.01119040982159, val loss: 48.58038587846412\n",
      "Iteration: 46, average loss: 51.89850370204391, val loss: 48.46583986449981\n",
      "Iteration: 47, average loss: 51.78625538268527, val loss: 48.35171367239645\n",
      "Iteration: 48, average loss: 51.674411303755164, val loss: 48.237979903591544\n",
      "Iteration: 49, average loss: 51.562967708145365, val loss: 48.12466372393847\n",
      "Iteration: 50, average loss: 51.45191708624651, val loss: 48.01173553057306\n",
      "Iteration: 51, average loss: 51.341253259907596, val loss: 47.89919133849553\n",
      "Iteration: 52, average loss: 51.23096103253572, val loss: 47.78702134058146\n",
      "Iteration: 53, average loss: 51.121053264912774, val loss: 47.675251958370126\n",
      "Iteration: 54, average loss: 51.011523200693915, val loss: 47.56383108529384\n",
      "Iteration: 55, average loss: 50.90235598651683, val loss: 47.45277580279763\n",
      "Iteration: 56, average loss: 50.793541317400724, val loss: 47.342068389843824\n",
      "Iteration: 57, average loss: 50.68508198410993, val loss: 47.2317285351756\n",
      "Iteration: 58, average loss: 50.57697317796053, val loss: 47.12170339228582\n",
      "Iteration: 59, average loss: 50.46918932827199, val loss: 47.01199186536572\n",
      "Iteration: 60, average loss: 50.36170989534129, val loss: 46.90258565780764\n",
      "Iteration: 61, average loss: 50.25453000252949, val loss: 46.79349916749134\n",
      "Iteration: 62, average loss: 50.1476408617508, val loss: 46.684712579709775\n",
      "Iteration: 63, average loss: 50.04106995794508, val loss: 46.576301829338405\n",
      "Iteration: 64, average loss: 49.934852236134994, val loss: 46.468265812775854\n",
      "Iteration: 65, average loss: 49.82901119494784, val loss: 46.36064564546151\n",
      "Iteration: 66, average loss: 49.723561255828194, val loss: 46.25341588669495\n",
      "Iteration: 67, average loss: 49.618512109857825, val loss: 46.146588223742995\n",
      "Iteration: 68, average loss: 49.5138495993499, val loss: 46.04010872746113\n",
      "Iteration: 69, average loss: 49.40954957146575, val loss: 45.933998293633074\n",
      "Iteration: 70, average loss: 49.305602116285314, val loss: 45.828240081887216\n",
      "Iteration: 71, average loss: 49.20202137306693, val loss: 45.722838633819386\n",
      "Iteration: 72, average loss: 49.09878689309825, val loss: 45.617775574242465\n",
      "Iteration: 73, average loss: 48.99590196356105, val loss: 45.51304771598907\n",
      "Iteration: 74, average loss: 48.893352650213934, val loss: 45.40865192000662\n",
      "Iteration: 75, average loss: 48.79114740251919, val loss: 45.304584587384795\n",
      "Iteration: 76, average loss: 48.689252261378336, val loss: 45.20079896007884\n",
      "Iteration: 77, average loss: 48.58767737397825, val loss: 45.09740916156566\n",
      "Iteration: 78, average loss: 48.486446858604175, val loss: 44.99439246418656\n",
      "Iteration: 79, average loss: 48.385582876666156, val loss: 44.8918380670119\n",
      "Iteration: 80, average loss: 48.285135405075145, val loss: 44.78971153876448\n",
      "Iteration: 81, average loss: 48.18510119235458, val loss: 44.688058827798315\n",
      "Iteration: 82, average loss: 48.08550229049535, val loss: 44.58685751859905\n",
      "Iteration: 83, average loss: 47.98634979920687, val loss: 44.486093404786665\n",
      "Iteration: 84, average loss: 47.88762826620093, val loss: 44.385745001907665\n",
      "Iteration: 85, average loss: 47.789337664986576, val loss: 44.28585844975548\n",
      "Iteration: 86, average loss: 47.691494823078024, val loss: 44.18644257632259\n",
      "Iteration: 87, average loss: 47.594105254048884, val loss: 44.087506244066944\n",
      "Iteration: 88, average loss: 47.49717183619882, val loss: 43.98902744363887\n",
      "Iteration: 89, average loss: 47.40068927709607, val loss: 43.8910177168458\n",
      "Iteration: 90, average loss: 47.3046659225427, val loss: 43.79348077220362\n",
      "Iteration: 91, average loss: 47.20910117706815, val loss: 43.69642189710894\n",
      "Iteration: 92, average loss: 47.113993728794334, val loss: 43.59982857183893\n",
      "Iteration: 93, average loss: 47.01932824637003, val loss: 43.50362726828624\n",
      "Iteration: 94, average loss: 46.92504248411759, val loss: 43.4078344052477\n",
      "Iteration: 95, average loss: 46.83119681607122, val loss: 43.312577118903505\n",
      "Iteration: 96, average loss: 46.737896417074154, val loss: 43.21790981402733\n",
      "Iteration: 97, average loss: 46.645169493080914, val loss: 43.12386487249964\n",
      "Iteration: 98, average loss: 46.55301967565564, val loss: 43.030433077475685\n",
      "Iteration: 99, average loss: 46.46145435231895, val loss: 42.937594190904\n",
      "Iteration: 100, average loss: 46.37046751192803, val loss: 42.845360641864474\n",
      "Iteration: 101, average loss: 46.28006448146802, val loss: 42.753730747778036\n",
      "Iteration: 102, average loss: 46.19024966995497, val loss: 42.66271724748263\n",
      "Iteration: 103, average loss: 46.10103318426344, val loss: 42.57234725447377\n",
      "Iteration: 104, average loss: 46.01242642932468, val loss: 42.48264451540826\n",
      "Iteration: 105, average loss: 45.92444075478448, val loss: 42.39360176338394\n",
      "Iteration: 106, average loss: 45.8370848197292, val loss: 42.305237492507864\n",
      "Iteration: 107, average loss: 45.75038796346545, val loss: 42.21763896506526\n",
      "Iteration: 108, average loss: 45.664403042355595, val loss: 42.13079023471221\n",
      "Iteration: 109, average loss: 45.57913188773077, val loss: 42.044675988686826\n",
      "Iteration: 110, average loss: 45.49456673083098, val loss: 41.95931645654184\n",
      "Iteration: 111, average loss: 45.41072015370724, val loss: 41.87472586225356\n",
      "Iteration: 112, average loss: 45.32760567595993, val loss: 41.79089453733634\n",
      "Iteration: 113, average loss: 45.24522883995719, val loss: 41.70783446548424\n",
      "Iteration: 114, average loss: 45.16359878622967, val loss: 41.62554966639703\n",
      "Iteration: 115, average loss: 45.08272521968049, val loss: 41.54404045840542\n",
      "Iteration: 116, average loss: 45.002606733985566, val loss: 41.463307566230206\n",
      "Iteration: 117, average loss: 44.923248906066455, val loss: 41.38335389497335\n",
      "Iteration: 118, average loss: 44.84465593301156, val loss: 41.304202043183246\n",
      "Iteration: 119, average loss: 44.766833956114915, val loss: 41.22584791778843\n",
      "Iteration: 120, average loss: 44.6897843817006, val loss: 41.14829253686922\n",
      "Iteration: 121, average loss: 44.61351209907716, val loss: 41.0715575467819\n",
      "Iteration: 122, average loss: 44.538025270913536, val loss: 40.9956524767392\n",
      "Iteration: 123, average loss: 44.463333395944126, val loss: 40.92058907967258\n",
      "Iteration: 124, average loss: 44.38944179539519, val loss: 40.84637632974086\n",
      "Iteration: 125, average loss: 44.31636430791035, val loss: 40.77303419473485\n",
      "Iteration: 126, average loss: 44.244122216091064, val loss: 40.700600096905475\n",
      "Iteration: 127, average loss: 44.17274718238536, val loss: 40.62907395976519\n",
      "Iteration: 128, average loss: 44.102245202962905, val loss: 40.55844852649976\n",
      "Iteration: 129, average loss: 44.03260752544311, val loss: 40.488713215326776\n",
      "Iteration: 130, average loss: 43.963826359182164, val loss: 40.41987175243995\n",
      "Iteration: 131, average loss: 43.895908349953984, val loss: 40.35191479726571\n",
      "Iteration: 132, average loss: 43.82885237707608, val loss: 40.28485950241547\n",
      "Iteration: 133, average loss: 43.76266493659089, val loss: 40.218700343268765\n",
      "Iteration: 134, average loss: 43.697346125247975, val loss: 40.15343941148875\n",
      "Iteration: 135, average loss: 43.63289777322668, val loss: 40.08908072503546\n",
      "Iteration: 136, average loss: 43.56932576497396, val loss: 40.0256320505816\n",
      "Iteration: 137, average loss: 43.50663147571582, val loss: 39.96310217433388\n",
      "Iteration: 138, average loss: 43.444816483391655, val loss: 39.90149749890732\n",
      "Iteration: 139, average loss: 43.38389247166362, val loss: 39.8408232973534\n",
      "Iteration: 140, average loss: 43.323863759708864, val loss: 39.78108068264249\n",
      "Iteration: 141, average loss: 43.264732086716066, val loss: 39.72227606572665\n",
      "Iteration: 142, average loss: 43.20649878414357, val loss: 39.66439964870192\n",
      "Iteration: 143, average loss: 43.14915930706522, val loss: 39.60744870253221\n",
      "Iteration: 144, average loss: 43.09270945724082, val loss: 39.55142082077616\n",
      "Iteration: 145, average loss: 43.0371457344092, val loss: 39.496314116069236\n",
      "Iteration: 146, average loss: 42.9824681546953, val loss: 39.44211786251312\n",
      "Iteration: 147, average loss: 42.92867195203109, val loss: 39.38883678836261\n",
      "Iteration: 148, average loss: 42.87575858341899, val loss: 39.33647841415606\n",
      "Iteration: 149, average loss: 42.823729240951906, val loss: 39.2850354377767\n",
      "Iteration: 150, average loss: 42.772579918736994, val loss: 39.234501008934146\n",
      "Iteration: 151, average loss: 42.722304205963574, val loss: 39.1848690903618\n",
      "Iteration: 152, average loss: 42.67290029894327, val loss: 39.136129637039595\n",
      "Iteration: 153, average loss: 42.624360128301355, val loss: 39.08828836842398\n",
      "Iteration: 154, average loss: 42.57668087563077, val loss: 39.041335555277456\n",
      "Iteration: 155, average loss: 42.52985751801643, val loss: 38.99527433159356\n",
      "Iteration: 156, average loss: 42.483892517965195, val loss: 38.950107310743405\n",
      "Iteration: 157, average loss: 42.43877889683857, val loss: 38.9058227111714\n",
      "Iteration: 158, average loss: 42.394513410070665, val loss: 38.86241394940962\n",
      "Iteration: 159, average loss: 42.351088079277446, val loss: 38.819878039989455\n",
      "Iteration: 160, average loss: 42.30849416359611, val loss: 38.77820653584947\n",
      "Iteration: 161, average loss: 42.26673554913433, val loss: 38.73739385732186\n",
      "Iteration: 162, average loss: 42.22579926799461, val loss: 38.697424548769334\n",
      "Iteration: 163, average loss: 42.18567395786157, val loss: 38.65828096609721\n",
      "Iteration: 164, average loss: 42.14634382436817, val loss: 38.6199391724847\n",
      "Iteration: 165, average loss: 42.107788755122016, val loss: 38.582349860996565\n",
      "Iteration: 166, average loss: 42.06997020808971, val loss: 38.54535515230769\n",
      "Iteration: 167, average loss: 42.03279592104004, val loss: 38.50863045699984\n",
      "Iteration: 168, average loss: 41.99603723673429, val loss: 38.4727930824716\n",
      "Iteration: 169, average loss: 41.96003278442051, val loss: 38.43847033528867\n",
      "Iteration: 170, average loss: 41.925276332431366, val loss: 38.404957199935694\n",
      "Iteration: 171, average loss: 41.8913149430556, val loss: 38.3721786757085\n",
      "Iteration: 172, average loss: 41.858085127844326, val loss: 38.34013010198488\n",
      "Iteration: 173, average loss: 41.82557413658658, val loss: 38.30881304579183\n",
      "Iteration: 174, average loss: 41.79377379739918, val loss: 38.27821203014828\n",
      "Iteration: 175, average loss: 41.762668522083814, val loss: 38.248310443230125\n",
      "Iteration: 176, average loss: 41.7322439490885, val loss: 38.21910186212137\n",
      "Iteration: 177, average loss: 41.70249065569634, val loss: 38.19057953508566\n",
      "Iteration: 178, average loss: 41.67340380442891, val loss: 38.16274859083259\n",
      "Iteration: 179, average loss: 41.64497340819686, val loss: 38.135608180590076\n",
      "Iteration: 180, average loss: 41.617194794226386, val loss: 38.109127969085094\n",
      "Iteration: 181, average loss: 41.59004776374154, val loss: 38.08329196638298\n",
      "Iteration: 182, average loss: 41.563518907712854, val loss: 38.05807613158915\n",
      "Iteration: 183, average loss: 41.53759672100417, val loss: 38.03348198893078\n",
      "Iteration: 184, average loss: 41.512268885322236, val loss: 38.00949521121926\n",
      "Iteration: 185, average loss: 41.48752639143939, val loss: 37.98611254769632\n",
      "Iteration: 186, average loss: 41.4633612183557, val loss: 37.96330649277235\n",
      "Iteration: 187, average loss: 41.43975908168848, val loss: 37.941078393196044\n",
      "Iteration: 188, average loss: 41.416707339494124, val loss: 37.91940825134462\n",
      "Iteration: 189, average loss: 41.39419475730491, val loss: 37.89827879161317\n",
      "Iteration: 190, average loss: 41.372211024381116, val loss: 37.877681451991535\n",
      "Iteration: 191, average loss: 41.35073909321845, val loss: 37.85758937359235\n",
      "Iteration: 192, average loss: 41.329764913245675, val loss: 37.83799788963318\n",
      "Iteration: 193, average loss: 41.30928231207069, val loss: 37.818890425252484\n",
      "Iteration: 194, average loss: 41.28927344630882, val loss: 37.80026067611864\n",
      "Iteration: 195, average loss: 41.269730687717306, val loss: 37.782089687628016\n",
      "Iteration: 196, average loss: 41.25063778352046, val loss: 37.76436584862351\n",
      "Iteration: 197, average loss: 41.23198414885479, val loss: 37.747096493245515\n",
      "Iteration: 198, average loss: 41.21376330495457, val loss: 37.73026301735307\n",
      "Iteration: 199, average loss: 41.195968581858466, val loss: 37.71386409466856\n",
      "Iteration: 200, average loss: 41.17859662673324, val loss: 37.697918207216944\n",
      "Iteration: 201, average loss: 41.16165512771422, val loss: 37.68239862721616\n",
      "Iteration: 202, average loss: 41.14512564594619, val loss: 37.667285403170425\n",
      "Iteration: 203, average loss: 41.12898830630353, val loss: 37.65256762849133\n",
      "Iteration: 204, average loss: 41.11323575351549, val loss: 37.63822786338524\n",
      "Iteration: 205, average loss: 41.097854196161464, val loss: 37.624247915622895\n",
      "Iteration: 206, average loss: 41.082826853950245, val loss: 37.61061893278249\n",
      "Iteration: 207, average loss: 41.06814043303043, val loss: 37.597328608084176\n",
      "Iteration: 208, average loss: 41.053781653371985, val loss: 37.58435876002638\n",
      "Iteration: 209, average loss: 41.039739790745976, val loss: 37.571707610771135\n",
      "Iteration: 210, average loss: 41.02600367518439, val loss: 37.559374461825996\n",
      "Iteration: 211, average loss: 41.01257430297741, val loss: 37.547359002974396\n",
      "Iteration: 212, average loss: 40.9994486230583, val loss: 37.53566390318859\n",
      "Iteration: 213, average loss: 40.986627625958356, val loss: 37.524271415906604\n",
      "Iteration: 214, average loss: 40.974097096401714, val loss: 37.513168734743935\n",
      "Iteration: 215, average loss: 40.961854294302384, val loss: 37.50235273845082\n",
      "Iteration: 216, average loss: 40.94989222259337, val loss: 37.49181303643709\n",
      "Iteration: 217, average loss: 40.93820046341938, val loss: 37.4815448237037\n",
      "Iteration: 218, average loss: 40.92676855746099, val loss: 37.47154028384416\n",
      "Iteration: 219, average loss: 40.915590175683946, val loss: 37.46179046366655\n",
      "Iteration: 220, average loss: 40.90466120507982, val loss: 37.452284033072125\n",
      "Iteration: 221, average loss: 40.893969431015606, val loss: 37.44301595643046\n",
      "Iteration: 222, average loss: 40.88350436307382, val loss: 37.43397130418027\n",
      "Iteration: 223, average loss: 40.873260576367954, val loss: 37.42514393639289\n",
      "Iteration: 224, average loss: 40.8632272913836, val loss: 37.41652204646559\n",
      "Iteration: 225, average loss: 40.853400066854874, val loss: 37.40810044508557\n",
      "Iteration: 226, average loss: 40.8437695376539, val loss: 37.39987484331385\n",
      "Iteration: 227, average loss: 40.83433353612964, val loss: 37.391840252746405\n",
      "Iteration: 228, average loss: 40.82508510898277, val loss: 37.38399271802569\n",
      "Iteration: 229, average loss: 40.816019236753526, val loss: 37.376300336834575\n",
      "Iteration: 230, average loss: 40.80711298057999, val loss: 37.36876767702708\n",
      "Iteration: 231, average loss: 40.79836496523613, val loss: 37.36142221019854\n",
      "Iteration: 232, average loss: 40.78980167476451, val loss: 37.354243343905196\n",
      "Iteration: 233, average loss: 40.78140261207802, val loss: 37.34723408335023\n",
      "Iteration: 234, average loss: 40.77316359275781, val loss: 37.340369854490135\n",
      "Iteration: 235, average loss: 40.7650707827674, val loss: 37.33363182461589\n",
      "Iteration: 236, average loss: 40.757108254133215, val loss: 37.32702805016693\n",
      "Iteration: 237, average loss: 40.74927282218196, val loss: 37.32053929836282\n",
      "Iteration: 238, average loss: 40.741552918429534, val loss: 37.31415272687379\n",
      "Iteration: 239, average loss: 40.73393921345328, val loss: 37.30785517834131\n",
      "Iteration: 240, average loss: 40.72641690226569, val loss: 37.301628335009646\n",
      "Iteration: 241, average loss: 40.71896749187783, val loss: 37.29541438053777\n",
      "Iteration: 242, average loss: 40.7115488063886, val loss: 37.28911674581277\n",
      "Iteration: 243, average loss: 40.70408633480901, val loss: 37.28258125711464\n",
      "Iteration: 244, average loss: 40.696446152700894, val loss: 37.275439985260746\n",
      "Iteration: 245, average loss: 40.688357080238454, val loss: 37.26703258315628\n",
      "Iteration: 246, average loss: 40.679242586744, val loss: 37.25725611528533\n",
      "Iteration: 247, average loss: 40.668760067022944, val loss: 37.248002236380806\n",
      "Iteration: 248, average loss: 40.65860499160877, val loss: 37.240711897414464\n",
      "Iteration: 249, average loss: 40.65018387462782, val loss: 37.23443967806309\n",
      "Iteration: 250, average loss: 40.64265871508686, val loss: 37.22864437176735\n",
      "Iteration: 251, average loss: 40.635517992259224, val loss: 37.22314431012561\n",
      "Iteration: 252, average loss: 40.62869191860807, val loss: 37.21789630312413\n",
      "Iteration: 253, average loss: 40.62214657534724, val loss: 37.21285512271943\n",
      "Iteration: 254, average loss: 40.61583013235083, val loss: 37.207957881018146\n",
      "Iteration: 255, average loss: 40.609659200705195, val loss: 37.203150167195815\n",
      "Iteration: 256, average loss: 40.603600349979125, val loss: 37.19842894790378\n",
      "Iteration: 257, average loss: 40.597634378838656, val loss: 37.193780411483786\n",
      "Iteration: 258, average loss: 40.59174528559625, val loss: 37.18919449546455\n",
      "Iteration: 259, average loss: 40.585924199237915, val loss: 37.18467154450519\n",
      "Iteration: 260, average loss: 40.580169396699915, val loss: 37.18021159967439\n",
      "Iteration: 261, average loss: 40.57447499007994, val loss: 37.17579848017532\n",
      "Iteration: 262, average loss: 40.56883826463119, val loss: 37.17143899018065\n",
      "Iteration: 263, average loss: 40.56326199384127, val loss: 37.167132612740744\n",
      "Iteration: 264, average loss: 40.55773750595424, val loss: 37.16287030208816\n",
      "Iteration: 265, average loss: 40.55226540335135, val loss: 37.158649518883614\n",
      "Iteration: 266, average loss: 40.54683797140628, val loss: 37.15448001339572\n",
      "Iteration: 267, average loss: 40.541457492948155, val loss: 37.15035443541858\n",
      "Iteration: 268, average loss: 40.53611793955743, val loss: 37.146254156133566\n",
      "Iteration: 269, average loss: 40.53081978807127, val loss: 37.142182892360154\n",
      "Iteration: 270, average loss: 40.52555990795007, val loss: 37.1381454210641\n",
      "Iteration: 271, average loss: 40.52033693318206, val loss: 37.13414469168274\n",
      "Iteration: 272, average loss: 40.515148541777606, val loss: 37.130168865828345\n",
      "Iteration: 273, average loss: 40.509991301430595, val loss: 37.12621866038823\n",
      "Iteration: 274, average loss: 40.50487458072423, val loss: 37.12229420229404\n",
      "Iteration: 275, average loss: 40.49979489437048, val loss: 37.118393398485054\n",
      "Iteration: 276, average loss: 40.49474727589151, val loss: 37.11452243481689\n",
      "Iteration: 277, average loss: 40.48973117588799, val loss: 37.11069099516625\n",
      "Iteration: 278, average loss: 40.48474366653369, val loss: 37.10688866066097\n",
      "Iteration: 279, average loss: 40.47978479620339, val loss: 37.10312736168709\n",
      "Iteration: 280, average loss: 40.47485823447002, val loss: 37.099406005542086\n",
      "Iteration: 281, average loss: 40.469956938195345, val loss: 37.09572069166929\n",
      "Iteration: 282, average loss: 40.46507894704883, val loss: 37.09206147849059\n",
      "Iteration: 283, average loss: 40.460220468217045, val loss: 37.0884127012088\n",
      "Iteration: 284, average loss: 40.455374502329434, val loss: 37.08474752112791\n",
      "Iteration: 285, average loss: 40.45054426976448, val loss: 37.0811166457443\n",
      "Iteration: 286, average loss: 40.445758069770925, val loss: 37.07753987330419\n",
      "Iteration: 287, average loss: 40.4410120934104, val loss: 37.07398919627711\n",
      "Iteration: 288, average loss: 40.43628890157322, val loss: 37.07045295767289\n",
      "Iteration: 289, average loss: 40.43158804045783, val loss: 37.066937384079765\n",
      "Iteration: 290, average loss: 40.426910684880426, val loss: 37.06344360206569\n",
      "Iteration: 291, average loss: 40.42225157120377, val loss: 37.05996551125942\n",
      "Iteration: 292, average loss: 40.4176080296005, val loss: 37.056505582332385\n",
      "Iteration: 293, average loss: 40.41298179580394, val loss: 37.05305734236535\n",
      "Iteration: 294, average loss: 40.40837166850694, val loss: 37.0496258976895\n",
      "Iteration: 295, average loss: 40.40377239904542, val loss: 37.04622824792883\n",
      "Iteration: 296, average loss: 40.399171551644514, val loss: 37.042833361417216\n",
      "Iteration: 297, average loss: 40.39453046563743, val loss: 37.03934700031185\n",
      "Iteration: 298, average loss: 40.38985067289233, val loss: 37.03586203522873\n",
      "Iteration: 299, average loss: 40.38522460149682, val loss: 37.032442041743934\n",
      "Iteration: 300, average loss: 40.38067462939571, val loss: 37.02905270870964\n",
      "Iteration: 301, average loss: 40.37615840331368, val loss: 37.02566993174931\n",
      "Iteration: 302, average loss: 40.37165922012882, val loss: 37.02229098309263\n",
      "Iteration: 303, average loss: 40.3671737673202, val loss: 37.0189256763568\n",
      "Iteration: 304, average loss: 40.36270267951892, val loss: 37.015565514501546\n",
      "Iteration: 305, average loss: 40.35823988568956, val loss: 37.012212759258865\n",
      "Iteration: 306, average loss: 40.35378105398537, val loss: 37.008869725500475\n",
      "Iteration: 307, average loss: 40.34932402021067, val loss: 37.00553495884854\n",
      "Iteration: 308, average loss: 40.34487387178025, val loss: 37.00220524336475\n",
      "Iteration: 309, average loss: 40.34042704278144, val loss: 36.998886180097564\n",
      "Iteration: 310, average loss: 40.33598946488422, val loss: 36.99556952430274\n",
      "Iteration: 311, average loss: 40.33155513616, val loss: 36.99225920130426\n",
      "Iteration: 312, average loss: 40.32712391493977, val loss: 36.988961350392216\n",
      "Iteration: 313, average loss: 40.32269943278769, val loss: 36.98567554481477\n",
      "Iteration: 314, average loss: 40.31828087424311, val loss: 36.98238685159743\n",
      "Iteration: 315, average loss: 40.313864299064676, val loss: 36.979101979176946\n",
      "Iteration: 316, average loss: 40.30945531296845, val loss: 36.97582047099944\n",
      "Iteration: 317, average loss: 40.305054085266185, val loss: 36.9725487734904\n",
      "Iteration: 318, average loss: 40.30066027503083, val loss: 36.96928716354668\n",
      "Iteration: 319, average loss: 40.29627396864591, val loss: 36.966016717948264\n",
      "Iteration: 320, average loss: 40.29189397632212, val loss: 36.962748943595145\n",
      "Iteration: 321, average loss: 40.2875188180214, val loss: 36.95947930889779\n",
      "Iteration: 322, average loss: 40.28314643436008, val loss: 36.95620263967248\n",
      "Iteration: 323, average loss: 40.278777542897465, val loss: 36.95292273543361\n",
      "Iteration: 324, average loss: 40.27440969379628, val loss: 36.94964830030225\n",
      "Iteration: 325, average loss: 40.27004341111667, val loss: 36.946385421851495\n",
      "Iteration: 326, average loss: 40.265676315279975, val loss: 36.943113836028026\n",
      "Iteration: 327, average loss: 40.26130820357281, val loss: 36.93983694007861\n",
      "Iteration: 328, average loss: 40.256942061410435, val loss: 36.93655276759956\n",
      "Iteration: 329, average loss: 40.25257221972885, val loss: 36.93326886543031\n",
      "Iteration: 330, average loss: 40.24820112030287, val loss: 36.929988197224496\n",
      "Iteration: 331, average loss: 40.24382761480727, val loss: 36.92670182522262\n",
      "Iteration: 332, average loss: 40.239453045066426, val loss: 36.92342168815886\n",
      "Iteration: 333, average loss: 40.235079925417324, val loss: 36.92014373069172\n",
      "Iteration: 334, average loss: 40.23070757055052, val loss: 36.9168639265453\n",
      "Iteration: 335, average loss: 40.226336464213865, val loss: 36.91358772344204\n",
      "Iteration: 336, average loss: 40.22196690356674, val loss: 36.91031711468672\n",
      "Iteration: 337, average loss: 40.217604149942815, val loss: 36.9070459565055\n",
      "Iteration: 338, average loss: 40.21324476642885, val loss: 36.903773411276255\n",
      "Iteration: 339, average loss: 40.2088880919028, val loss: 36.90050283747951\n",
      "Iteration: 340, average loss: 40.204537410091085, val loss: 36.89723493590064\n",
      "Iteration: 341, average loss: 40.20018881760934, val loss: 36.8939695880132\n",
      "Iteration: 342, average loss: 40.19584391312899, val loss: 36.89070451866068\n",
      "Iteration: 343, average loss: 40.19150259759691, val loss: 36.887442152228594\n",
      "Iteration: 344, average loss: 40.187162197730395, val loss: 36.884181845501764\n",
      "Iteration: 345, average loss: 40.18282287362693, val loss: 36.88091969468759\n",
      "Iteration: 346, average loss: 40.17848820271699, val loss: 36.877658666975876\n",
      "Iteration: 347, average loss: 40.1741586998465, val loss: 36.87439804250006\n",
      "Iteration: 348, average loss: 40.16983074612088, val loss: 36.871145730675195\n",
      "Iteration: 349, average loss: 40.1655060807288, val loss: 36.867897405252066\n",
      "Iteration: 350, average loss: 40.16118376381731, val loss: 36.86465913910587\n",
      "Iteration: 351, average loss: 40.1568633634687, val loss: 36.86141839227879\n",
      "Iteration: 352, average loss: 40.15254061003238, val loss: 36.858174386232704\n",
      "Iteration: 353, average loss: 40.148215934274276, val loss: 36.854929339029546\n",
      "Iteration: 354, average loss: 40.14388718697184, val loss: 36.851685529626025\n",
      "Iteration: 355, average loss: 40.13955841663379, val loss: 36.84843861610201\n",
      "Iteration: 356, average loss: 40.135232015508386, val loss: 36.845193152441254\n",
      "Iteration: 357, average loss: 40.13090867927109, val loss: 36.84194635158222\n",
      "Iteration: 358, average loss: 40.1265856689877, val loss: 36.83869916881815\n",
      "Iteration: 359, average loss: 40.122263196585834, val loss: 36.8354486099906\n",
      "Iteration: 360, average loss: 40.117942113231344, val loss: 36.83220367719008\n",
      "Iteration: 361, average loss: 40.113624414959965, val loss: 36.828961198853754\n",
      "Iteration: 362, average loss: 40.109306380368665, val loss: 36.825715329049004\n",
      "Iteration: 363, average loss: 40.104989502165054, val loss: 36.82246699835818\n",
      "Iteration: 364, average loss: 40.10066978712589, val loss: 36.81921160961217\n",
      "Iteration: 365, average loss: 40.09634647507598, val loss: 36.81594273168698\n",
      "Iteration: 366, average loss: 40.09201999217416, val loss: 36.81267390357665\n",
      "Iteration: 367, average loss: 40.087692759463174, val loss: 36.809398671307726\n",
      "Iteration: 368, average loss: 40.083364123883456, val loss: 36.806117667555704\n",
      "Iteration: 369, average loss: 40.07903602503348, val loss: 36.802845483307124\n",
      "Iteration: 370, average loss: 40.07471016639673, val loss: 36.79957244196768\n",
      "Iteration: 371, average loss: 40.07038082707907, val loss: 36.796291926595345\n",
      "Iteration: 372, average loss: 40.066044552890574, val loss: 36.79299618756933\n",
      "Iteration: 373, average loss: 40.061701008663086, val loss: 36.789698136548694\n",
      "Iteration: 374, average loss: 40.05735373381831, val loss: 36.786402266614324\n",
      "Iteration: 375, average loss: 40.05300182996741, val loss: 36.78310104855893\n",
      "Iteration: 376, average loss: 40.048652244650796, val loss: 36.77981401860378\n",
      "Iteration: 377, average loss: 40.04431058128099, val loss: 36.77653625582023\n",
      "Iteration: 378, average loss: 40.03996952029242, val loss: 36.77325893679174\n",
      "Iteration: 379, average loss: 40.03562921141657, val loss: 36.769979615463306\n",
      "Iteration: 380, average loss: 40.03128672913077, val loss: 36.766696300321044\n",
      "Iteration: 381, average loss: 40.026943954292705, val loss: 36.76341011371804\n",
      "Iteration: 382, average loss: 40.02260036629755, val loss: 36.76011973312946\n",
      "Iteration: 383, average loss: 40.018255287898334, val loss: 36.75683043008092\n",
      "Iteration: 384, average loss: 40.01391201894641, val loss: 36.75354888957445\n",
      "Iteration: 385, average loss: 40.00957332371514, val loss: 36.75027397203662\n",
      "Iteration: 386, average loss: 40.005235241231134, val loss: 36.74699836166134\n",
      "Iteration: 387, average loss: 40.000895978172046, val loss: 36.74372078308077\n",
      "Iteration: 388, average loss: 39.99655695698687, val loss: 36.74043769343062\n",
      "Iteration: 389, average loss: 39.992215404188, val loss: 36.73715541642437\n",
      "Iteration: 390, average loss: 39.98787558712245, val loss: 36.733869840640054\n",
      "Iteration: 391, average loss: 39.98353371758392, val loss: 36.7305827808225\n",
      "Iteration: 392, average loss: 39.97918871289866, val loss: 36.727295091266\n",
      "Iteration: 393, average loss: 39.97484230995178, val loss: 36.72400171942099\n",
      "Iteration: 394, average loss: 39.97049258987685, val loss: 36.72070634642231\n",
      "Iteration: 395, average loss: 39.96614139897812, val loss: 36.71741223002042\n",
      "Iteration: 396, average loss: 39.96178769604595, val loss: 36.71411247964785\n",
      "Iteration: 397, average loss: 39.957430404165514, val loss: 36.71081290752455\n",
      "Iteration: 398, average loss: 39.95307113237427, val loss: 36.707511962583105\n",
      "Iteration: 399, average loss: 39.94871138144231, val loss: 36.704206558190855\n",
      "Iteration: 400, average loss: 39.944350566265086, val loss: 36.70090116263357\n",
      "Iteration: 401, average loss: 39.93998869836042, val loss: 36.69759371525429\n",
      "Iteration: 402, average loss: 39.93562454762666, val loss: 36.694282700605385\n",
      "Iteration: 403, average loss: 39.93125851257987, val loss: 36.690968372205695\n",
      "Iteration: 404, average loss: 39.9268890026111, val loss: 36.68764663702845\n",
      "Iteration: 405, average loss: 39.922516470370084, val loss: 36.684324145646016\n",
      "Iteration: 406, average loss: 39.91814052540323, val loss: 36.681001342225954\n",
      "Iteration: 407, average loss: 39.91376264774857, val loss: 36.67768064361503\n",
      "Iteration: 408, average loss: 39.90938106827114, val loss: 36.67436064455889\n",
      "Iteration: 409, average loss: 39.904996320245345, val loss: 36.671033830441615\n",
      "Iteration: 410, average loss: 39.9006080650477, val loss: 36.667708782160055\n",
      "Iteration: 411, average loss: 39.89621715729939, val loss: 36.66437715940481\n",
      "Iteration: 412, average loss: 39.891822157274696, val loss: 36.66103979987988\n",
      "Iteration: 413, average loss: 39.88742330569576, val loss: 36.657701337932636\n",
      "Iteration: 414, average loss: 39.88302006468105, val loss: 36.65435924352308\n",
      "Iteration: 415, average loss: 39.87861224073143, val loss: 36.65101204142201\n",
      "Iteration: 416, average loss: 39.87420037979089, val loss: 36.6476636656556\n",
      "Iteration: 417, average loss: 39.86978520517764, val loss: 36.64431437163783\n",
      "Iteration: 418, average loss: 39.8653676878427, val loss: 36.640959700953196\n",
      "Iteration: 419, average loss: 39.86094617958806, val loss: 36.63759783494781\n",
      "Iteration: 420, average loss: 39.85651975092681, val loss: 36.634237114802566\n",
      "Iteration: 421, average loss: 39.852089846191774, val loss: 36.63087389962696\n",
      "Iteration: 422, average loss: 39.847656275339176, val loss: 36.62750853955142\n",
      "Iteration: 423, average loss: 39.843220474639374, val loss: 36.62414202314589\n",
      "Iteration: 424, average loss: 39.83878115179458, val loss: 36.62076835649408\n",
      "Iteration: 425, average loss: 39.83433884007919, val loss: 36.61739065692569\n",
      "Iteration: 426, average loss: 39.829892882978285, val loss: 36.614010690678825\n",
      "Iteration: 427, average loss: 39.825443022493005, val loss: 36.61063110504328\n",
      "Iteration: 428, average loss: 39.82098905706175, val loss: 36.60724781290314\n",
      "Iteration: 429, average loss: 39.81653273393567, val loss: 36.603857258831894\n",
      "Iteration: 430, average loss: 39.81207174033934, val loss: 36.60046227047\n",
      "Iteration: 431, average loss: 39.80760655310995, val loss: 36.59706876391503\n",
      "Iteration: 432, average loss: 39.80313711465845, val loss: 36.593669723052415\n",
      "Iteration: 433, average loss: 39.79866368759082, val loss: 36.59026913848841\n",
      "Iteration: 434, average loss: 39.794188024917084, val loss: 36.58686855945655\n",
      "Iteration: 435, average loss: 39.78970840357352, val loss: 36.58346326508061\n",
      "Iteration: 436, average loss: 39.78522540060219, val loss: 36.58005261245098\n",
      "Iteration: 437, average loss: 39.78073968173225, val loss: 36.57663034125216\n",
      "Iteration: 438, average loss: 39.776251537212424, val loss: 36.57320806703534\n",
      "Iteration: 439, average loss: 39.77176011702865, val loss: 36.56977873273037\n",
      "Iteration: 440, average loss: 39.76726545803789, val loss: 36.56635145679995\n",
      "Iteration: 441, average loss: 39.76276798179184, val loss: 36.562922193006884\n",
      "Iteration: 442, average loss: 39.758268585527574, val loss: 36.55949783889764\n",
      "Iteration: 443, average loss: 39.75376602186673, val loss: 36.55606773904255\n",
      "Iteration: 444, average loss: 39.74925832702342, val loss: 36.55263223671891\n",
      "Iteration: 445, average loss: 39.74474839772579, val loss: 36.54919124276968\n",
      "Iteration: 446, average loss: 39.74023428055399, val loss: 36.5457503624114\n",
      "Iteration: 447, average loss: 39.73571672416539, val loss: 36.54230383782916\n",
      "Iteration: 448, average loss: 39.73119395827326, val loss: 36.53885261747815\n",
      "Iteration: 449, average loss: 39.7266677658339, val loss: 36.535396346673814\n",
      "Iteration: 450, average loss: 39.72213860640779, val loss: 36.531935388825254\n",
      "Iteration: 451, average loss: 39.71760571751618, val loss: 36.52847619189568\n",
      "Iteration: 452, average loss: 39.713069170569455, val loss: 36.52501821368273\n",
      "Iteration: 453, average loss: 39.70852895980872, val loss: 36.52155912334023\n",
      "Iteration: 454, average loss: 39.70398586268586, val loss: 36.5180963082321\n",
      "Iteration: 455, average loss: 39.69943678667004, val loss: 36.51462696272397\n",
      "Iteration: 456, average loss: 39.69488334079871, val loss: 36.51116110067302\n",
      "Iteration: 457, average loss: 39.690327169814545, val loss: 36.507692203315955\n",
      "Iteration: 458, average loss: 39.68576593560297, val loss: 36.504223050305576\n",
      "Iteration: 459, average loss: 39.68119995490365, val loss: 36.50074955891891\n",
      "Iteration: 460, average loss: 39.676629772508775, val loss: 36.49726576509126\n",
      "Iteration: 461, average loss: 39.67205432647668, val loss: 36.49377343090596\n",
      "Iteration: 462, average loss: 39.667475336415755, val loss: 36.49027766307695\n",
      "Iteration: 463, average loss: 39.66289303728924, val loss: 36.486779059928494\n",
      "Iteration: 464, average loss: 39.65830772625652, val loss: 36.48327541842372\n",
      "Iteration: 465, average loss: 39.65371863392816, val loss: 36.47977212145652\n",
      "Iteration: 466, average loss: 39.64912912811058, val loss: 36.47627069974046\n",
      "Iteration: 467, average loss: 39.64453539525829, val loss: 36.472767620823525\n",
      "Iteration: 468, average loss: 39.63993813335032, val loss: 36.46926027303362\n",
      "Iteration: 469, average loss: 39.6353364152033, val loss: 36.46574768467949\n",
      "Iteration: 470, average loss: 39.63072955665957, val loss: 36.46223007250792\n",
      "Iteration: 471, average loss: 39.62611941438942, val loss: 36.45870718974561\n",
      "Iteration: 472, average loss: 39.6215035661983, val loss: 36.4551821772386\n",
      "Iteration: 473, average loss: 39.61688458286046, val loss: 36.45165433118405\n",
      "Iteration: 474, average loss: 39.612261622424285, val loss: 36.44812452515306\n",
      "Iteration: 475, average loss: 39.60763470907718, val loss: 36.4445871683984\n",
      "Iteration: 476, average loss: 39.603004315049176, val loss: 36.441048255822125\n",
      "Iteration: 477, average loss: 39.598369131917536, val loss: 36.437506921832735\n",
      "Iteration: 478, average loss: 39.593729208057056, val loss: 36.433957865455675\n",
      "Iteration: 479, average loss: 39.5890859497918, val loss: 36.43040609279836\n",
      "Iteration: 480, average loss: 39.584438077493566, val loss: 36.42684798036929\n",
      "Iteration: 481, average loss: 39.57978457874722, val loss: 36.423288941729155\n",
      "Iteration: 482, average loss: 39.5751264118342, val loss: 36.41972152511987\n",
      "Iteration: 483, average loss: 39.57046314138145, val loss: 36.41614917708481\n",
      "Iteration: 484, average loss: 39.565795752161364, val loss: 36.412576274340864\n",
      "Iteration: 485, average loss: 39.56112376157788, val loss: 36.4090021000495\n",
      "Iteration: 486, average loss: 39.55644795514535, val loss: 36.40542517916187\n",
      "Iteration: 487, average loss: 39.551768533273595, val loss: 36.40184314659375\n",
      "Iteration: 488, average loss: 39.54708489473315, val loss: 36.3982631783909\n",
      "Iteration: 489, average loss: 39.54239552032544, val loss: 36.3946800693209\n",
      "Iteration: 490, average loss: 39.53770253393385, val loss: 36.39109526600853\n",
      "Iteration: 491, average loss: 39.53300466053728, val loss: 36.387503315572744\n",
      "Iteration: 492, average loss: 39.528303421637865, val loss: 36.38390540964298\n",
      "Iteration: 493, average loss: 39.523596510219114, val loss: 36.38030479783815\n",
      "Iteration: 494, average loss: 39.51888626669916, val loss: 36.37669951765495\n",
      "Iteration: 495, average loss: 39.51417214743757, val loss: 36.37309026535522\n",
      "Iteration: 496, average loss: 39.50945360879391, val loss: 36.36947932943813\n",
      "Iteration: 497, average loss: 39.50473027759128, val loss: 36.36586131877451\n",
      "Iteration: 498, average loss: 39.50000332979764, val loss: 36.362238680383385\n",
      "Iteration: 499, average loss: 39.495271494999024, val loss: 36.358609183377176\n",
      "Iteration: 500, average loss: 39.49053486879321, val loss: 36.354977004620146\n",
      "Iteration: 501, average loss: 39.485792347774414, val loss: 36.35134123847341\n",
      "Iteration: 502, average loss: 39.48104454699346, val loss: 36.34769758329254\n",
      "Iteration: 503, average loss: 39.47629089056006, val loss: 36.34405509093081\n",
      "Iteration: 504, average loss: 39.47153376611534, val loss: 36.34040833025133\n",
      "Iteration: 505, average loss: 39.46677117532002, val loss: 36.336761304899866\n",
      "Iteration: 506, average loss: 39.462002655158294, val loss: 36.33311110877928\n",
      "Iteration: 507, average loss: 39.45722902569794, val loss: 36.32945624397844\n",
      "Iteration: 508, average loss: 39.452452643482005, val loss: 36.32579524114415\n",
      "Iteration: 509, average loss: 39.44767088705791, val loss: 36.32213365727076\n",
      "Iteration: 510, average loss: 39.44288542996282, val loss: 36.3184639381084\n",
      "Iteration: 511, average loss: 39.43809411491173, val loss: 36.314791608511754\n",
      "Iteration: 512, average loss: 39.43329796698934, val loss: 36.31111552123417\n",
      "Iteration: 513, average loss: 39.42849498094568, val loss: 36.30743596785617\n",
      "Iteration: 514, average loss: 39.423686226784895, val loss: 36.30375323874999\n",
      "Iteration: 515, average loss: 39.41887232186138, val loss: 36.30006543358837\n",
      "Iteration: 516, average loss: 39.41405209826962, val loss: 36.29637354964453\n",
      "Iteration: 517, average loss: 39.409227304412546, val loss: 36.29267923088754\n",
      "Iteration: 518, average loss: 39.40439829849391, val loss: 36.288976030041\n",
      "Iteration: 519, average loss: 39.399563622359494, val loss: 36.28526374919508\n",
      "Iteration: 520, average loss: 39.39472244442373, val loss: 36.28154636925738\n",
      "Iteration: 521, average loss: 39.3898767998829, val loss: 36.277824780126494\n",
      "Iteration: 522, average loss: 39.38502585715142, val loss: 36.274096846982715\n",
      "Iteration: 523, average loss: 39.38016892055383, val loss: 36.27036632182366\n",
      "Iteration: 524, average loss: 39.37530672953324, val loss: 36.26663547077247\n",
      "Iteration: 525, average loss: 39.37044071114582, val loss: 36.262895443693154\n",
      "Iteration: 526, average loss: 39.365570345938494, val loss: 36.25915051823962\n",
      "Iteration: 527, average loss: 39.36069411586448, val loss: 36.255402875608276\n",
      "Iteration: 528, average loss: 39.35581398355788, val loss: 36.251650636932915\n",
      "Iteration: 529, average loss: 39.35092560565414, val loss: 36.24789237202195\n",
      "Iteration: 530, average loss: 39.34603165889132, val loss: 36.24412977259956\n",
      "Iteration: 531, average loss: 39.34113241163429, val loss: 36.240359145509544\n",
      "Iteration: 532, average loss: 39.33622634468447, val loss: 36.236588162644935\n",
      "Iteration: 533, average loss: 39.33131470772379, val loss: 36.23281433124104\n",
      "Iteration: 534, average loss: 39.326398892103185, val loss: 36.22903691587462\n",
      "Iteration: 535, average loss: 39.32147748804323, val loss: 36.225253045763566\n",
      "Iteration: 536, average loss: 39.316549256228015, val loss: 36.22146737065543\n",
      "Iteration: 537, average loss: 39.31161630902314, val loss: 36.217678380619404\n",
      "Iteration: 538, average loss: 39.30667621156444, val loss: 36.21388350145204\n",
      "Iteration: 539, average loss: 39.30172817603402, val loss: 36.210087733384924\n",
      "Iteration: 540, average loss: 39.2967744242166, val loss: 36.20628749697829\n",
      "Iteration: 541, average loss: 39.291815192227205, val loss: 36.202481094622925\n",
      "Iteration: 542, average loss: 39.28685075534138, val loss: 36.19867663875565\n",
      "Iteration: 543, average loss: 39.281881107800245, val loss: 36.1948655106312\n",
      "Iteration: 544, average loss: 39.27690476495862, val loss: 36.191047861042826\n",
      "Iteration: 545, average loss: 39.271922982257344, val loss: 36.18722276896834\n",
      "Iteration: 546, average loss: 39.26693757720616, val loss: 36.183389127511006\n",
      "Iteration: 547, average loss: 39.26194552292571, val loss: 36.17955102909262\n",
      "Iteration: 548, average loss: 39.25694830290937, val loss: 36.175709848708735\n",
      "Iteration: 549, average loss: 39.25194613254013, val loss: 36.17186802779816\n",
      "Iteration: 550, average loss: 39.24693907286234, val loss: 36.16801684619764\n",
      "Iteration: 551, average loss: 39.24192599628282, val loss: 36.16416207894297\n",
      "Iteration: 552, average loss: 39.23690697075664, val loss: 36.16030635376704\n",
      "Iteration: 553, average loss: 39.23188243511218, val loss: 36.156447103028796\n",
      "Iteration: 554, average loss: 39.22685342480019, val loss: 36.15258515977464\n",
      "Iteration: 555, average loss: 39.22181918655617, val loss: 36.14871593692746\n",
      "Iteration: 556, average loss: 39.21677962132698, val loss: 36.14484500018468\n",
      "Iteration: 557, average loss: 39.21173529348512, val loss: 36.14097273939327\n",
      "Iteration: 558, average loss: 39.20668541521266, val loss: 36.137091827951856\n",
      "Iteration: 559, average loss: 39.20163003373261, val loss: 36.133203102443886\n",
      "Iteration: 560, average loss: 39.19656850404785, val loss: 36.129308284733405\n",
      "Iteration: 561, average loss: 39.191500765114014, val loss: 36.12540674000416\n",
      "Iteration: 562, average loss: 39.18642696551079, val loss: 36.12150498491469\n",
      "Iteration: 563, average loss: 39.18134924639826, val loss: 36.11760251900085\n",
      "Iteration: 564, average loss: 39.17626430677331, val loss: 36.113696360595085\n",
      "Iteration: 565, average loss: 39.17117299204288, val loss: 36.10978433416109\n",
      "Iteration: 566, average loss: 39.16607403985544, val loss: 36.10586805373206\n",
      "Iteration: 567, average loss: 39.16096844304587, val loss: 36.101949375932136\n",
      "Iteration: 568, average loss: 39.15585712879752, val loss: 36.09802449432859\n",
      "Iteration: 569, average loss: 39.150741259256996, val loss: 36.09409472217363\n",
      "Iteration: 570, average loss: 39.14561922999396, val loss: 36.09016616311602\n",
      "Iteration: 571, average loss: 39.14049306123153, val loss: 36.086229778909555\n",
      "Iteration: 572, average loss: 39.1353601211511, val loss: 36.08229346666517\n",
      "Iteration: 573, average loss: 39.130222291762124, val loss: 36.07835223195216\n",
      "Iteration: 574, average loss: 39.12507929894083, val loss: 36.074403539428886\n",
      "Iteration: 575, average loss: 39.11993191092486, val loss: 36.070447743734235\n",
      "Iteration: 576, average loss: 39.11477740029782, val loss: 36.06648507424453\n",
      "Iteration: 577, average loss: 39.1096161287188, val loss: 36.06251752815452\n",
      "Iteration: 578, average loss: 39.104450466552215, val loss: 36.058552619710056\n",
      "Iteration: 579, average loss: 39.099279260865735, val loss: 36.054580020669064\n",
      "Iteration: 580, average loss: 39.094103351307375, val loss: 36.050602249366456\n",
      "Iteration: 581, average loss: 39.088920693466626, val loss: 36.04661768804088\n",
      "Iteration: 582, average loss: 39.08373594399236, val loss: 36.042630481533976\n",
      "Iteration: 583, average loss: 39.07854395327361, val loss: 36.03863726569645\n",
      "Iteration: 584, average loss: 39.07334447137399, val loss: 36.03464742586549\n",
      "Iteration: 585, average loss: 39.068138492280156, val loss: 36.03065671740642\n",
      "Iteration: 586, average loss: 39.06292685103301, val loss: 36.02665661011374\n",
      "Iteration: 587, average loss: 39.05771105646511, val loss: 36.02265309614149\n",
      "Iteration: 588, average loss: 39.05248838116005, val loss: 36.01864322078728\n",
      "Iteration: 589, average loss: 39.04725819854921, val loss: 36.01462951472102\n",
      "Iteration: 590, average loss: 39.04202182166242, val loss: 36.01061043221956\n",
      "Iteration: 591, average loss: 39.03678078236787, val loss: 36.006587195457676\n",
      "Iteration: 592, average loss: 39.03153319865609, val loss: 36.00255243712196\n",
      "Iteration: 593, average loss: 39.026277978639094, val loss: 35.99851662809836\n",
      "Iteration: 594, average loss: 39.02101629713307, val loss: 35.99447884774994\n",
      "Iteration: 595, average loss: 39.01574835339606, val loss: 35.99043617150607\n",
      "Iteration: 596, average loss: 39.010474160097644, val loss: 35.986379505930245\n",
      "Iteration: 597, average loss: 39.0051933694001, val loss: 35.98231867021122\n",
      "Iteration: 598, average loss: 38.99990573136703, val loss: 35.97825207357094\n",
      "Iteration: 599, average loss: 38.99461047891258, val loss: 35.974179233077464\n",
      "Iteration: 600, average loss: 38.98930892045947, val loss: 35.97010114424152\n",
      "Iteration: 601, average loss: 38.98400398498572, val loss: 35.96601788238568\n",
      "Iteration: 602, average loss: 38.978693297519776, val loss: 35.961931198049896\n",
      "Iteration: 603, average loss: 38.97337536996113, val loss: 35.95784141880337\n",
      "Iteration: 604, average loss: 38.96805185857026, val loss: 35.953747970253076\n",
      "Iteration: 605, average loss: 38.96272189720817, val loss: 35.94965295179658\n",
      "Iteration: 606, average loss: 38.957386965912896, val loss: 35.94555077349472\n",
      "Iteration: 607, average loss: 38.95204625843804, val loss: 35.94144189852175\n",
      "Iteration: 608, average loss: 38.94669881189503, val loss: 35.93733171304136\n",
      "Iteration: 609, average loss: 38.94134525861141, val loss: 35.9332167363092\n",
      "Iteration: 610, average loss: 38.93598523232096, val loss: 35.92909705900883\n",
      "Iteration: 611, average loss: 38.93062064382765, val loss: 35.92496722727288\n",
      "Iteration: 612, average loss: 38.92524996471866, val loss: 35.92083595848209\n",
      "Iteration: 613, average loss: 38.91987466236243, val loss: 35.916701264163166\n",
      "Iteration: 614, average loss: 38.914494024958586, val loss: 35.912566384748615\n",
      "Iteration: 615, average loss: 38.909107935025496, val loss: 35.90842614525796\n",
      "Iteration: 616, average loss: 38.90371736351418, val loss: 35.90428387963969\n",
      "Iteration: 617, average loss: 38.898320143925396, val loss: 35.90013641930433\n",
      "Iteration: 618, average loss: 38.89291720113893, val loss: 35.895988133946275\n",
      "Iteration: 619, average loss: 38.887509334490495, val loss: 35.89182962435642\n",
      "Iteration: 620, average loss: 38.882093954777375, val loss: 35.88766787210973\n",
      "Iteration: 621, average loss: 38.876672773545494, val loss: 35.88350401984572\n",
      "Iteration: 622, average loss: 38.87124482560273, val loss: 35.87933626867029\n",
      "Iteration: 623, average loss: 38.86581043805477, val loss: 35.87516089736418\n",
      "Iteration: 624, average loss: 38.8603696120534, val loss: 35.87097424338197\n",
      "Iteration: 625, average loss: 38.854923129657614, val loss: 35.86678402637845\n",
      "Iteration: 626, average loss: 38.849469877095615, val loss: 35.86259063186138\n",
      "Iteration: 627, average loss: 38.84400915638836, val loss: 35.85839217655347\n",
      "Iteration: 628, average loss: 38.8385429808483, val loss: 35.854190036667234\n",
      "Iteration: 629, average loss: 38.83307004550805, val loss: 35.84998023271409\n",
      "Iteration: 630, average loss: 38.827590010592324, val loss: 35.845763227829636\n",
      "Iteration: 631, average loss: 38.8221036788922, val loss: 35.84154296045911\n",
      "Iteration: 632, average loss: 38.81661031672344, val loss: 35.83731886678423\n",
      "Iteration: 633, average loss: 38.8111109883313, val loss: 35.83308590732672\n",
      "Iteration: 634, average loss: 38.80560398792875, val loss: 35.828845659639335\n",
      "Iteration: 635, average loss: 38.800089629951884, val loss: 35.82460202696001\n",
      "Iteration: 636, average loss: 38.79456768519636, val loss: 35.82035340845665\n",
      "Iteration: 637, average loss: 38.78903916262198, val loss: 35.81610221161479\n",
      "Iteration: 638, average loss: 38.78350381574769, val loss: 35.81184510921427\n",
      "Iteration: 639, average loss: 38.777960492792914, val loss: 35.80758631316278\n",
      "Iteration: 640, average loss: 38.77240916381135, val loss: 35.80331854954198\n",
      "Iteration: 641, average loss: 38.766850371291675, val loss: 35.799046879137755\n",
      "Iteration: 642, average loss: 38.76128431794724, val loss: 35.7947669161682\n",
      "Iteration: 643, average loss: 38.755710181406734, val loss: 35.790484389942584\n",
      "Iteration: 644, average loss: 38.75012865734561, val loss: 35.78619294130501\n",
      "Iteration: 645, average loss: 38.74453867345616, val loss: 35.78189618419076\n",
      "Iteration: 646, average loss: 38.73894144717046, val loss: 35.77759618975283\n",
      "Iteration: 647, average loss: 38.7333364152678, val loss: 35.77328631225031\n",
      "Iteration: 648, average loss: 38.72772367219418, val loss: 35.76896938749935\n",
      "Iteration: 649, average loss: 38.722102562586464, val loss: 35.76464747169338\n",
      "Iteration: 650, average loss: 38.716474212886055, val loss: 35.76031672152438\n",
      "Iteration: 651, average loss: 38.710837346920066, val loss: 35.75598258628149\n",
      "Iteration: 652, average loss: 38.70519313604935, val loss: 35.751639453659905\n",
      "Iteration: 653, average loss: 38.69954139944436, val loss: 35.74729011748333\n",
      "Iteration: 654, average loss: 38.69388110510969, val loss: 35.74293299228283\n",
      "Iteration: 655, average loss: 38.688212554811855, val loss: 35.73857127904603\n",
      "Iteration: 656, average loss: 38.6825369083939, val loss: 35.734199680993996\n",
      "Iteration: 657, average loss: 38.676851317502454, val loss: 35.729820764388165\n",
      "Iteration: 658, average loss: 38.67115807072552, val loss: 35.725438423308844\n",
      "Iteration: 659, average loss: 38.66545673844895, val loss: 35.72105039761904\n",
      "Iteration: 660, average loss: 38.659747233137416, val loss: 35.71665750956735\n",
      "Iteration: 661, average loss: 38.654028861419015, val loss: 35.71225665989735\n",
      "Iteration: 662, average loss: 38.64830209667556, val loss: 35.70784793779926\n",
      "Iteration: 663, average loss: 38.64256760809157, val loss: 35.703434863014806\n",
      "Iteration: 664, average loss: 38.63682471496472, val loss: 35.69901587592002\n",
      "Iteration: 665, average loss: 38.6310738146593, val loss: 35.69458794706944\n",
      "Iteration: 666, average loss: 38.62531442688283, val loss: 35.69015250798686\n",
      "Iteration: 667, average loss: 38.61954733484609, val loss: 35.68571169425174\n",
      "Iteration: 668, average loss: 38.61377136488468, val loss: 35.681266087059896\n",
      "Iteration: 669, average loss: 38.60798810760756, val loss: 35.676816242220916\n",
      "Iteration: 670, average loss: 38.602195684460625, val loss: 35.67235906808183\n",
      "Iteration: 671, average loss: 38.596393607088906, val loss: 35.6678978168654\n",
      "Iteration: 672, average loss: 38.59058332328059, val loss: 35.66342655199581\n",
      "Iteration: 673, average loss: 38.584764986222496, val loss: 35.65895155924389\n",
      "Iteration: 674, average loss: 38.57893814902375, val loss: 35.65446602210996\n",
      "Iteration: 675, average loss: 38.573102129830254, val loss: 35.64997552408813\n",
      "Iteration: 676, average loss: 38.56725806429766, val loss: 35.645475407014544\n",
      "Iteration: 677, average loss: 38.56140502063548, val loss: 35.640972173427926\n",
      "Iteration: 678, average loss: 38.55554281571061, val loss: 35.636464018674\n",
      "Iteration: 679, average loss: 38.54967223273383, val loss: 35.63194538760315\n",
      "Iteration: 680, average loss: 38.54379340646348, val loss: 35.627422969319525\n",
      "Iteration: 681, average loss: 38.53790573221474, val loss: 35.622897471137534\n",
      "Iteration: 682, average loss: 38.53200850394613, val loss: 35.61836292260513\n",
      "Iteration: 683, average loss: 38.52610274789414, val loss: 35.613822267141046\n",
      "Iteration: 684, average loss: 38.52018861494203, val loss: 35.60926590685235\n",
      "Iteration: 685, average loss: 38.51426496137167, val loss: 35.60471032125919\n",
      "Iteration: 686, average loss: 38.508332986186666, val loss: 35.60014782000739\n",
      "Iteration: 687, average loss: 38.50239229317449, val loss: 35.59557456307296\n",
      "Iteration: 688, average loss: 38.496443406395294, val loss: 35.59100137600896\n",
      "Iteration: 689, average loss: 38.49048468916888, val loss: 35.58642032033072\n",
      "Iteration: 690, average loss: 38.48451854065421, val loss: 35.581832519628186\n",
      "Iteration: 691, average loss: 38.478542627343806, val loss: 35.57723684859395\n",
      "Iteration: 692, average loss: 38.472558193160715, val loss: 35.5726337797896\n",
      "Iteration: 693, average loss: 38.46656484074063, val loss: 35.56802429924834\n",
      "Iteration: 694, average loss: 38.4605625965745, val loss: 35.56340874481775\n",
      "Iteration: 695, average loss: 38.454551294805924, val loss: 35.55878829564361\n",
      "Iteration: 696, average loss: 38.44853098035435, val loss: 35.55416316491519\n",
      "Iteration: 697, average loss: 38.44250196304874, val loss: 35.54952595180107\n",
      "Iteration: 698, average loss: 38.43646275594039, val loss: 35.544875698242485\n",
      "Iteration: 699, average loss: 38.4304144013907, val loss: 35.54022055882391\n",
      "Iteration: 700, average loss: 38.424355929600445, val loss: 35.53556129895003\n",
      "Iteration: 701, average loss: 38.418289303203714, val loss: 35.53089636445789\n",
      "Iteration: 702, average loss: 38.41221260563763, val loss: 35.52622863400144\n",
      "Iteration: 703, average loss: 38.40612736877036, val loss: 35.5215473212395\n",
      "Iteration: 704, average loss: 38.40003264583827, val loss: 35.51686148585234\n",
      "Iteration: 705, average loss: 38.393927799906706, val loss: 35.512168058306926\n",
      "Iteration: 706, average loss: 38.387813829569424, val loss: 35.507463410249485\n",
      "Iteration: 707, average loss: 38.381690609282344, val loss: 35.50275538957985\n",
      "Iteration: 708, average loss: 38.37555836133911, val loss: 35.498044024772305\n",
      "Iteration: 709, average loss: 38.369416483358485, val loss: 35.49332010654288\n",
      "Iteration: 710, average loss: 38.3632653531245, val loss: 35.48858629574049\n",
      "Iteration: 711, average loss: 38.3571032660019, val loss: 35.48384553241828\n",
      "Iteration: 712, average loss: 38.35093150852959, val loss: 35.479103467717394\n",
      "Iteration: 713, average loss: 38.344749827315844, val loss: 35.47434777963821\n",
      "Iteration: 714, average loss: 38.33855864621591, val loss: 35.46958625563648\n",
      "Iteration: 715, average loss: 38.332358074648944, val loss: 35.46481844035432\n",
      "Iteration: 716, average loss: 38.326147769384335, val loss: 35.46004289517522\n",
      "Iteration: 717, average loss: 38.31992801721545, val loss: 35.45525419571655\n",
      "Iteration: 718, average loss: 38.31369681980299, val loss: 35.45045914865997\n",
      "Iteration: 719, average loss: 38.30745659588616, val loss: 35.44565619559465\n",
      "Iteration: 720, average loss: 38.301206290433946, val loss: 35.44084626672014\n",
      "Iteration: 721, average loss: 38.294945888473215, val loss: 35.43603035007449\n",
      "Iteration: 722, average loss: 38.28867458606112, val loss: 35.43120961022236\n",
      "Iteration: 723, average loss: 38.28239407516332, val loss: 35.42637643670007\n",
      "Iteration: 724, average loss: 38.27610260852869, val loss: 35.42153825727794\n",
      "Iteration: 725, average loss: 38.269801104126344, val loss: 35.41669235864284\n",
      "Iteration: 726, average loss: 38.2634895965097, val loss: 35.411841486758206\n",
      "Iteration: 727, average loss: 38.257168266508316, val loss: 35.40698166563131\n",
      "Iteration: 728, average loss: 38.25083596694873, val loss: 35.40211522459713\n",
      "Iteration: 729, average loss: 38.24449428613635, val loss: 35.39723919522732\n",
      "Iteration: 730, average loss: 38.23814146645403, val loss: 35.392354068666364\n",
      "Iteration: 731, average loss: 38.23177856869168, val loss: 35.38746884580659\n",
      "Iteration: 732, average loss: 38.22540511140501, val loss: 35.38257705217425\n",
      "Iteration: 733, average loss: 38.21902041849883, val loss: 35.37767836990831\n",
      "Iteration: 734, average loss: 38.2126254286743, val loss: 35.3727661184581\n",
      "Iteration: 735, average loss: 38.20621949002363, val loss: 35.36784216568317\n",
      "Iteration: 736, average loss: 38.19980213837923, val loss: 35.36291152275529\n",
      "Iteration: 737, average loss: 38.19337476854739, val loss: 35.35796309123902\n",
      "Iteration: 738, average loss: 38.186936744745225, val loss: 35.35300702265535\n",
      "Iteration: 739, average loss: 38.18048842402472, val loss: 35.348048528838646\n",
      "Iteration: 740, average loss: 38.17402900705015, val loss: 35.34307651788782\n",
      "Iteration: 741, average loss: 38.16755886469486, val loss: 35.33809997672744\n",
      "Iteration: 742, average loss: 38.16107818239553, val loss: 35.3331140926169\n",
      "Iteration: 743, average loss: 38.15458659273415, val loss: 35.328122739895335\n",
      "Iteration: 744, average loss: 38.14808479138618, val loss: 35.32312361160794\n",
      "Iteration: 745, average loss: 38.14157222319341, val loss: 35.318119289765974\n",
      "Iteration: 746, average loss: 38.1350492797612, val loss: 35.31310182384902\n",
      "Iteration: 747, average loss: 38.12851497401362, val loss: 35.308079730213755\n",
      "Iteration: 748, average loss: 38.121970143295144, val loss: 35.30305135554589\n",
      "Iteration: 749, average loss: 38.11541409308208, val loss: 35.29800992313751\n",
      "Iteration: 750, average loss: 38.108847874950094, val loss: 35.292962672943645\n",
      "Iteration: 751, average loss: 38.102270832384264, val loss: 35.28790661968496\n",
      "Iteration: 752, average loss: 38.095682525404406, val loss: 35.2828414875777\n",
      "Iteration: 753, average loss: 38.08908372109639, val loss: 35.27777829782146\n",
      "Iteration: 754, average loss: 38.082474154550674, val loss: 35.27270112434157\n",
      "Iteration: 755, average loss: 38.07585258875492, val loss: 35.26761659663469\n",
      "Iteration: 756, average loss: 38.06922031370338, val loss: 35.262520963056495\n",
      "Iteration: 757, average loss: 38.0625771485665, val loss: 35.257416181809994\n",
      "Iteration: 758, average loss: 38.055922611899994, val loss: 35.25229584302822\n",
      "Iteration: 759, average loss: 38.04925697437231, val loss: 35.247176709046194\n",
      "Iteration: 760, average loss: 38.042580575758706, val loss: 35.24204090610897\n",
      "Iteration: 761, average loss: 38.03589345176439, val loss: 35.23690125905829\n",
      "Iteration: 762, average loss: 38.02919512785575, val loss: 35.23174928951487\n",
      "Iteration: 763, average loss: 38.02248609584311, val loss: 35.22659067140083\n",
      "Iteration: 764, average loss: 38.015766377610284, val loss: 35.22141935489158\n",
      "Iteration: 765, average loss: 38.009035611498184, val loss: 35.21624624627612\n",
      "Iteration: 766, average loss: 38.00229351647234, val loss: 35.21107262124324\n",
      "Iteration: 767, average loss: 37.99553999002429, val loss: 35.205893912635894\n",
      "Iteration: 768, average loss: 37.98877487896721, val loss: 35.20069327734037\n",
      "Iteration: 769, average loss: 37.98199847815693, val loss: 35.195492787679065\n",
      "Iteration: 770, average loss: 37.97521145332263, val loss: 35.19027605846431\n",
      "Iteration: 771, average loss: 37.96841342552848, val loss: 35.18504727728983\n",
      "Iteration: 772, average loss: 37.961603711768625, val loss: 35.17981661902833\n",
      "Iteration: 773, average loss: 37.95478308028069, val loss: 35.174581309787506\n",
      "Iteration: 774, average loss: 37.94794991511654, val loss: 35.16933131733021\n",
      "Iteration: 775, average loss: 37.941106645381396, val loss: 35.164074040712315\n",
      "Iteration: 776, average loss: 37.9342524682842, val loss: 35.158802784462516\n",
      "Iteration: 777, average loss: 37.927385948706366, val loss: 35.15351765161948\n",
      "Iteration: 778, average loss: 37.92050771091295, val loss: 35.14823560766011\n",
      "Iteration: 779, average loss: 37.91361831466933, val loss: 35.14293793064998\n",
      "Iteration: 780, average loss: 37.906716693426674, val loss: 35.1376312448275\n",
      "Iteration: 781, average loss: 37.89980275389077, val loss: 35.13232243919138\n",
      "Iteration: 782, average loss: 37.89287701205931, val loss: 35.127000895858636\n",
      "Iteration: 783, average loss: 37.885939251397545, val loss: 35.12166968381379\n",
      "Iteration: 784, average loss: 37.878989186263894, val loss: 35.11633914937465\n",
      "Iteration: 785, average loss: 37.87202848558841, val loss: 35.11099286506722\n",
      "Iteration: 786, average loss: 37.86505530421861, val loss: 35.10564278253131\n",
      "Iteration: 787, average loss: 37.85807012590233, val loss: 35.10028234679433\n",
      "Iteration: 788, average loss: 37.85107314644228, val loss: 35.09491822578004\n",
      "Iteration: 789, average loss: 37.84406363791314, val loss: 35.089540970320506\n",
      "Iteration: 790, average loss: 37.83704167863597, val loss: 35.08414598413263\n",
      "Iteration: 791, average loss: 37.83000750357402, val loss: 35.078752800359744\n",
      "Iteration: 792, average loss: 37.82296159301979, val loss: 35.07334225923564\n",
      "Iteration: 793, average loss: 37.81590320177124, val loss: 35.067930207085624\n",
      "Iteration: 794, average loss: 37.80883305429836, val loss: 35.06249966434329\n",
      "Iteration: 795, average loss: 37.801750836165056, val loss: 35.05706383792964\n",
      "Iteration: 796, average loss: 37.79465569159835, val loss: 35.05161756010178\n",
      "Iteration: 797, average loss: 37.787549342510204, val loss: 35.04615770425777\n",
      "Iteration: 798, average loss: 37.7804301119081, val loss: 35.04069349079999\n",
      "Iteration: 799, average loss: 37.77329928863452, val loss: 35.035224344314216\n",
      "Iteration: 800, average loss: 37.76615563106998, val loss: 35.02975010884255\n",
      "Iteration: 801, average loss: 37.75899956076617, val loss: 35.02426259287082\n",
      "Iteration: 802, average loss: 37.75183107887489, val loss: 35.01876174801572\n",
      "Iteration: 803, average loss: 37.74464986520113, val loss: 35.0132617254357\n",
      "Iteration: 804, average loss: 37.73745694943672, val loss: 35.00773929924603\n",
      "Iteration: 805, average loss: 37.73025118786356, val loss: 35.002202501513395\n",
      "Iteration: 806, average loss: 37.72303292947115, val loss: 34.996663796610605\n",
      "Iteration: 807, average loss: 37.7158034550395, val loss: 34.99112501238122\n",
      "Iteration: 808, average loss: 37.70856095512132, val loss: 34.98557093713425\n",
      "Iteration: 809, average loss: 37.7013053179939, val loss: 34.98000705531581\n",
      "Iteration: 810, average loss: 37.694036816629236, val loss: 34.9744361665317\n",
      "Iteration: 811, average loss: 37.68675591404311, val loss: 34.968846255745035\n",
      "Iteration: 812, average loss: 37.679460459861204, val loss: 34.96324918938679\n",
      "Iteration: 813, average loss: 37.67215315616073, val loss: 34.957645363836846\n",
      "Iteration: 814, average loss: 37.66483261965323, val loss: 34.95203307466788\n",
      "Iteration: 815, average loss: 37.65749822722541, val loss: 34.94642286363631\n",
      "Iteration: 816, average loss: 37.650151122595375, val loss: 34.94079443227355\n",
      "Iteration: 817, average loss: 37.6427900330456, val loss: 34.93516020979978\n",
      "Iteration: 818, average loss: 37.63541603779447, val loss: 34.92950752640179\n",
      "Iteration: 819, average loss: 37.62802757387576, val loss: 34.92384256747046\n",
      "Iteration: 820, average loss: 37.62062805286352, val loss: 34.91817579524275\n",
      "Iteration: 821, average loss: 37.61321413920121, val loss: 34.91250652532775\n",
      "Iteration: 822, average loss: 37.60578779321938, val loss: 34.90681837543048\n",
      "Iteration: 823, average loss: 37.59834782512868, val loss: 34.90111265256356\n",
      "Iteration: 824, average loss: 37.59089610081364, val loss: 34.89539035432472\n",
      "Iteration: 825, average loss: 37.58343219872258, val loss: 34.88966811891566\n",
      "Iteration: 826, average loss: 37.57595398230253, val loss: 34.88393064059261\n",
      "Iteration: 827, average loss: 37.56846256878065, val loss: 34.87818756945921\n",
      "Iteration: 828, average loss: 37.56095794663913, val loss: 34.87242302646142\n",
      "Iteration: 829, average loss: 37.55344037618038, val loss: 34.866650101002335\n",
      "Iteration: 830, average loss: 37.54590888875694, val loss: 34.860867876205766\n",
      "Iteration: 831, average loss: 37.53836360530577, val loss: 34.85508326020904\n",
      "Iteration: 832, average loss: 37.53080503606566, val loss: 34.84929118355148\n",
      "Iteration: 833, average loss: 37.523232111032456, val loss: 34.84349814710538\n",
      "Iteration: 834, average loss: 37.51564696100023, val loss: 34.83769070309255\n",
      "Iteration: 835, average loss: 37.508047466692716, val loss: 34.83186352896107\n",
      "Iteration: 836, average loss: 37.50043377899318, val loss: 34.82603270606761\n",
      "Iteration: 837, average loss: 37.49280707617313, val loss: 34.820192370411135\n",
      "Iteration: 838, average loss: 37.48516529424179, val loss: 34.814343735100564\n",
      "Iteration: 839, average loss: 37.47750970706848, val loss: 34.80848508380309\n",
      "Iteration: 840, average loss: 37.469839418567894, val loss: 34.802600434608514\n",
      "Iteration: 841, average loss: 37.46215419608038, val loss: 34.796723997909154\n",
      "Iteration: 842, average loss: 37.454455365305364, val loss: 34.79083347615891\n",
      "Iteration: 843, average loss: 37.4467414289281, val loss: 34.78491754390082\n",
      "Iteration: 844, average loss: 37.4390124986713, val loss: 34.77899277660384\n",
      "Iteration: 845, average loss: 37.431269866832785, val loss: 34.7730681299819\n",
      "Iteration: 846, average loss: 37.423513526501864, val loss: 34.767132626350936\n",
      "Iteration: 847, average loss: 37.41574226024646, val loss: 34.761177152534216\n",
      "Iteration: 848, average loss: 37.407956933053796, val loss: 34.755199252882115\n",
      "Iteration: 849, average loss: 37.40015783171723, val loss: 34.74921609708262\n",
      "Iteration: 850, average loss: 37.39234380791153, val loss: 34.74321764686694\n",
      "Iteration: 851, average loss: 37.384515146126496, val loss: 34.73721361616757\n",
      "Iteration: 852, average loss: 37.376671262409374, val loss: 34.73119277801734\n",
      "Iteration: 853, average loss: 37.368812903113984, val loss: 34.725164430357836\n",
      "Iteration: 854, average loss: 37.36093935413637, val loss: 34.71913404917463\n",
      "Iteration: 855, average loss: 37.35305212430908, val loss: 34.713084054006224\n",
      "Iteration: 856, average loss: 37.34514839292149, val loss: 34.70702566071776\n",
      "Iteration: 857, average loss: 37.33722969299353, val loss: 34.700965453288156\n",
      "Iteration: 858, average loss: 37.32929482782521, val loss: 34.69488586508449\n",
      "Iteration: 859, average loss: 37.321344585234414, val loss: 34.68880828590274\n",
      "Iteration: 860, average loss: 37.31337778694964, val loss: 34.68270114013451\n",
      "Iteration: 861, average loss: 37.30539468981794, val loss: 34.676584244363255\n",
      "Iteration: 862, average loss: 37.29739526734836, val loss: 34.67046221920427\n",
      "Iteration: 863, average loss: 37.289379934181916, val loss: 34.66432297371041\n",
      "Iteration: 864, average loss: 37.281347660626764, val loss: 34.658174326439166\n",
      "Iteration: 865, average loss: 37.27329914235838, val loss: 34.65201489828269\n",
      "Iteration: 866, average loss: 37.265235298497664, val loss: 34.645826766280436\n",
      "Iteration: 867, average loss: 37.2571550210317, val loss: 34.63963730679252\n",
      "Iteration: 868, average loss: 37.24905773407019, val loss: 34.633433550409634\n",
      "Iteration: 869, average loss: 37.24094407339603, val loss: 34.62721828565198\n",
      "Iteration: 870, average loss: 37.23281456767649, val loss: 34.620994579563686\n",
      "Iteration: 871, average loss: 37.224668309308484, val loss: 34.61476460634681\n",
      "Iteration: 872, average loss: 37.2165052867742, val loss: 34.60853114074111\n",
      "Iteration: 873, average loss: 37.20832537107422, val loss: 34.60227948624607\n",
      "Iteration: 874, average loss: 37.200128220129706, val loss: 34.596010071865045\n",
      "Iteration: 875, average loss: 37.19191612252867, val loss: 34.58973019446224\n",
      "Iteration: 876, average loss: 37.18368979813396, val loss: 34.58343293089719\n",
      "Iteration: 877, average loss: 37.17544844185097, val loss: 34.57712495836223\n",
      "Iteration: 878, average loss: 37.167192633025316, val loss: 34.570802063611424\n",
      "Iteration: 879, average loss: 37.158920322639354, val loss: 34.56448068316674\n",
      "Iteration: 880, average loss: 37.150632117681454, val loss: 34.55814390893879\n",
      "Iteration: 881, average loss: 37.14232636534649, val loss: 34.55179323376457\n",
      "Iteration: 882, average loss: 37.134003414623976, val loss: 34.54541800708627\n",
      "Iteration: 883, average loss: 37.125665685404904, val loss: 34.53902485296517\n",
      "Iteration: 884, average loss: 37.11731113212696, val loss: 34.532599532112506\n",
      "Iteration: 885, average loss: 37.10893916047138, val loss: 34.5261661305483\n",
      "Iteration: 886, average loss: 37.100545955740884, val loss: 34.51971519080772\n",
      "Iteration: 887, average loss: 37.09213040301189, val loss: 34.51323673764667\n",
      "Iteration: 888, average loss: 37.08368856549839, val loss: 34.50674316394302\n",
      "Iteration: 889, average loss: 37.07522088202877, val loss: 34.50022211304248\n",
      "Iteration: 890, average loss: 37.0667223803663, val loss: 34.49367189160667\n",
      "Iteration: 891, average loss: 37.05819603901554, val loss: 34.48711251258281\n",
      "Iteration: 892, average loss: 37.0496521456806, val loss: 34.480547158841354\n",
      "Iteration: 893, average loss: 37.04109690500342, val loss: 34.47398040536435\n",
      "Iteration: 894, average loss: 37.03253906015037, val loss: 34.467408868549306\n",
      "Iteration: 895, average loss: 37.023973974052836, val loss: 34.46084146441455\n",
      "Iteration: 896, average loss: 37.01540091533016, val loss: 34.45426884622565\n",
      "Iteration: 897, average loss: 37.00681804689232, val loss: 34.44769289365535\n",
      "Iteration: 898, average loss: 36.998223621488194, val loss: 34.441110664466486\n",
      "Iteration: 899, average loss: 36.989618443060614, val loss: 34.434525411730846\n",
      "Iteration: 900, average loss: 36.98100101659839, val loss: 34.42792588745429\n",
      "Iteration: 901, average loss: 36.972372082696445, val loss: 34.42132063667519\n",
      "Iteration: 902, average loss: 36.96373315249088, val loss: 34.4147061235705\n",
      "Iteration: 903, average loss: 36.955082976299785, val loss: 34.40808017880508\n",
      "Iteration: 904, average loss: 36.9464202169059, val loss: 34.40144731083827\n",
      "Iteration: 905, average loss: 36.93774400356311, val loss: 34.394805676654286\n",
      "Iteration: 906, average loss: 36.92905270535013, val loss: 34.388148760258744\n",
      "Iteration: 907, average loss: 36.9203458511887, val loss: 34.38148315298629\n",
      "Iteration: 908, average loss: 36.91162321187448, val loss: 34.37480889322997\n",
      "Iteration: 909, average loss: 36.90288487148746, val loss: 34.36811331056634\n",
      "Iteration: 910, average loss: 36.894128677349734, val loss: 34.361403956812\n",
      "Iteration: 911, average loss: 36.88535582616134, val loss: 34.35467659455636\n",
      "Iteration: 912, average loss: 36.87656561072898, val loss: 34.34794430299434\n",
      "Iteration: 913, average loss: 36.86775969652738, val loss: 34.34119670001088\n",
      "Iteration: 914, average loss: 36.858935836432636, val loss: 34.33442963986323\n",
      "Iteration: 915, average loss: 36.8500962983007, val loss: 34.3276516481404\n",
      "Iteration: 916, average loss: 36.84123859889265, val loss: 34.32086191881466\n",
      "Iteration: 917, average loss: 36.83236497266281, val loss: 34.314060163792604\n",
      "Iteration: 918, average loss: 36.82347590681435, val loss: 34.30725063697422\n",
      "Iteration: 919, average loss: 36.814569026375736, val loss: 34.300423352776775\n",
      "Iteration: 920, average loss: 36.80564587127759, val loss: 34.29358123359004\n",
      "Iteration: 921, average loss: 36.796705764272936, val loss: 34.28673533265745\n",
      "Iteration: 922, average loss: 36.7877505539696, val loss: 34.27987067250139\n",
      "Iteration: 923, average loss: 36.778777089095925, val loss: 34.272998164400626\n",
      "Iteration: 924, average loss: 36.76978705931401, val loss: 34.266109128638284\n",
      "Iteration: 925, average loss: 36.760779415351756, val loss: 34.25920790718113\n",
      "Iteration: 926, average loss: 36.75175399020098, val loss: 34.252293155112184\n",
      "Iteration: 927, average loss: 36.742711487599614, val loss: 34.245367703025956\n",
      "Iteration: 928, average loss: 36.733651503272675, val loss: 34.23842856794083\n",
      "Iteration: 929, average loss: 36.72457354656164, val loss: 34.231471659585296\n",
      "Iteration: 930, average loss: 36.71547747925284, val loss: 34.22450607983639\n",
      "Iteration: 931, average loss: 36.7063660921106, val loss: 34.21753083397561\n",
      "Iteration: 932, average loss: 36.69723591942718, val loss: 34.21053107590871\n",
      "Iteration: 933, average loss: 36.688086917434916, val loss: 34.20352151486961\n",
      "Iteration: 934, average loss: 36.67892152445328, val loss: 34.19649723199633\n",
      "Iteration: 935, average loss: 36.669738660112095, val loss: 34.18946122354375\n",
      "Iteration: 936, average loss: 36.66053800997527, val loss: 34.18240712258934\n",
      "Iteration: 937, average loss: 36.65132049777082, val loss: 34.17534169499182\n",
      "Iteration: 938, average loss: 36.642084832352715, val loss: 34.16826803148702\n",
      "Iteration: 939, average loss: 36.632832079118, val loss: 34.161179224638104\n",
      "Iteration: 940, average loss: 36.62356093425106, val loss: 34.15407782285475\n",
      "Iteration: 941, average loss: 36.61427228807827, val loss: 34.14695538063297\n",
      "Iteration: 942, average loss: 36.60496411231405, val loss: 34.13982488976139\n",
      "Iteration: 943, average loss: 36.595638749680084, val loss: 34.13267854252503\n",
      "Iteration: 944, average loss: 36.5862943066491, val loss: 34.12551758140948\n",
      "Iteration: 945, average loss: 36.57693152957492, val loss: 34.11834936080885\n",
      "Iteration: 946, average loss: 36.567550400029056, val loss: 34.111160672425996\n",
      "Iteration: 947, average loss: 36.558148961136304, val loss: 34.10396837206493\n",
      "Iteration: 948, average loss: 36.54873092738902, val loss: 34.09675725754297\n",
      "Iteration: 949, average loss: 36.53929395376196, val loss: 34.089529429216924\n",
      "Iteration: 950, average loss: 36.529838701377166, val loss: 34.082289546166976\n",
      "Iteration: 951, average loss: 36.520366372693566, val loss: 34.07503617950513\n",
      "Iteration: 952, average loss: 36.510876594534245, val loss: 34.06776423654034\n",
      "Iteration: 953, average loss: 36.50136752750563, val loss: 34.0604823515266\n",
      "Iteration: 954, average loss: 36.4918416744269, val loss: 34.05318475739964\n",
      "Iteration: 955, average loss: 36.48229720281518, val loss: 34.045872474572235\n",
      "Iteration: 956, average loss: 36.472734088483065, val loss: 34.038546865126875\n",
      "Iteration: 957, average loss: 36.463153050141635, val loss: 34.031207953141184\n",
      "Iteration: 958, average loss: 36.453553294214075, val loss: 34.02385887112358\n",
      "Iteration: 959, average loss: 36.44393502802089, val loss: 34.01649386347402\n",
      "Iteration: 960, average loss: 36.43429758468112, val loss: 34.00911252817412\n",
      "Iteration: 961, average loss: 36.42464306619432, val loss: 34.00171877608328\n",
      "Iteration: 962, average loss: 36.414967836389216, val loss: 33.99431015588359\n",
      "Iteration: 963, average loss: 36.405273180652934, val loss: 33.98689314475679\n",
      "Iteration: 964, average loss: 36.3955596299563, val loss: 33.97945729401819\n",
      "Iteration: 965, average loss: 36.38582736512889, val loss: 33.97200645113636\n",
      "Iteration: 966, average loss: 36.37607598995817, val loss: 33.96453880727786\n",
      "Iteration: 967, average loss: 36.3663049020629, val loss: 33.95705719671038\n",
      "Iteration: 968, average loss: 36.35651558608824, val loss: 33.94956740798226\n",
      "Iteration: 969, average loss: 36.34670867436174, val loss: 33.94206160726871\n",
      "Iteration: 970, average loss: 36.336882485283745, val loss: 33.934539284394205\n",
      "Iteration: 971, average loss: 36.32703643605329, val loss: 33.92700561563491\n",
      "Iteration: 972, average loss: 36.317171893833915, val loss: 33.919458762183915\n",
      "Iteration: 973, average loss: 36.30728774139847, val loss: 33.91189501377661\n",
      "Iteration: 974, average loss: 36.29738436228987, val loss: 33.90431578200963\n",
      "Iteration: 975, average loss: 36.287461529607356, val loss: 33.89671953450339\n",
      "Iteration: 976, average loss: 36.27751751568007, val loss: 33.88911038209017\n",
      "Iteration: 977, average loss: 36.26755156609171, val loss: 33.88148720998244\n",
      "Iteration: 978, average loss: 36.25756593257333, val loss: 33.87384794524138\n",
      "Iteration: 979, average loss: 36.24755805010957, val loss: 33.86619371167778\n",
      "Iteration: 980, average loss: 36.23752679110725, val loss: 33.858522299732094\n",
      "Iteration: 981, average loss: 36.227472034629415, val loss: 33.850834376495456\n",
      "Iteration: 982, average loss: 36.217392964063635, val loss: 33.843128440738425\n",
      "Iteration: 983, average loss: 36.20729010116651, val loss: 33.83540590497462\n",
      "Iteration: 984, average loss: 36.19716263623629, val loss: 33.82766449507419\n",
      "Iteration: 985, average loss: 36.18700746061721, val loss: 33.81990203199267\n",
      "Iteration: 986, average loss: 36.17682491408454, val loss: 33.812116483669854\n",
      "Iteration: 987, average loss: 36.16661311232525, val loss: 33.80430942408453\n",
      "Iteration: 988, average loss: 36.15637816323174, val loss: 33.79649062744086\n",
      "Iteration: 989, average loss: 36.146122981960644, val loss: 33.788661801301785\n",
      "Iteration: 990, average loss: 36.13585068638198, val loss: 33.78081017800969\n",
      "Iteration: 991, average loss: 36.12555873797135, val loss: 33.772946094220046\n",
      "Iteration: 992, average loss: 36.11524975472602, val loss: 33.76506781341188\n",
      "Iteration: 993, average loss: 36.10492510150596, val loss: 33.75717510252493\n",
      "Iteration: 994, average loss: 36.094584749516656, val loss: 33.74927364633484\n",
      "Iteration: 995, average loss: 36.0842297549409, val loss: 33.74136162480321\n",
      "Iteration: 996, average loss: 36.07385850758944, val loss: 33.73343037279309\n",
      "Iteration: 997, average loss: 36.06347005839509, val loss: 33.72549210285411\n",
      "Iteration: 998, average loss: 36.053066279001285, val loss: 33.717544802538654\n",
      "Iteration: 999, average loss: 36.042645989984706, val loss: 33.7095852047809\n",
      "Iteration: 1000, average loss: 36.032206048136175, val loss: 33.70160779298078\n",
      "Iteration: 1001, average loss: 36.0217444366879, val loss: 33.693615059736864\n",
      "Iteration: 1002, average loss: 36.01126202408242, val loss: 33.68560646971648\n",
      "Iteration: 1003, average loss: 36.000758213697424, val loss: 33.677577768855116\n",
      "Iteration: 1004, average loss: 35.99023158308388, val loss: 33.66953632308727\n",
      "Iteration: 1005, average loss: 35.97968425266985, val loss: 33.66147668971402\n",
      "Iteration: 1006, average loss: 35.969114368088576, val loss: 33.65339755629485\n",
      "Iteration: 1007, average loss: 35.958523033897656, val loss: 33.64530743393531\n",
      "Iteration: 1008, average loss: 35.9479116068946, val loss: 33.637201719633524\n",
      "Iteration: 1009, average loss: 35.93728089102225, val loss: 33.629081870379935\n",
      "Iteration: 1010, average loss: 35.92663030347963, val loss: 33.62094625608932\n",
      "Iteration: 1011, average loss: 35.915960650513135, val loss: 33.6127977828903\n",
      "Iteration: 1012, average loss: 35.905271667213256, val loss: 33.60463689781265\n",
      "Iteration: 1013, average loss: 35.89456430955786, val loss: 33.596463356858465\n",
      "Iteration: 1014, average loss: 35.88383721038339, val loss: 33.58827564066566\n",
      "Iteration: 1015, average loss: 35.87309083270566, val loss: 33.58007331164468\n",
      "Iteration: 1016, average loss: 35.86232297432019, val loss: 33.57185607958094\n",
      "Iteration: 1017, average loss: 35.85153497935493, val loss: 33.56362270177017\n",
      "Iteration: 1018, average loss: 35.84072664624827, val loss: 33.55537513457855\n",
      "Iteration: 1019, average loss: 35.82989748549346, val loss: 33.54711005415143\n",
      "Iteration: 1020, average loss: 35.81904683596846, val loss: 33.538832875090485\n",
      "Iteration: 1021, average loss: 35.80817528623314, val loss: 33.53053455857475\n",
      "Iteration: 1022, average loss: 35.797281571632425, val loss: 33.52222415973061\n",
      "Iteration: 1023, average loss: 35.78636695682139, val loss: 33.51389328806425\n",
      "Iteration: 1024, average loss: 35.775430708115806, val loss: 33.505546495159656\n",
      "Iteration: 1025, average loss: 35.76447281745321, val loss: 33.49718390657597\n",
      "Iteration: 1026, average loss: 35.753492962335045, val loss: 33.48880645312616\n",
      "Iteration: 1027, average loss: 35.74249138693879, val loss: 33.48041273330871\n",
      "Iteration: 1028, average loss: 35.731467844783396, val loss: 33.472000061379845\n",
      "Iteration: 1029, average loss: 35.720420717617166, val loss: 33.46357528788558\n",
      "Iteration: 1030, average loss: 35.709351856351475, val loss: 33.45513324774093\n",
      "Iteration: 1031, average loss: 35.698260177160805, val loss: 33.44667245461592\n",
      "Iteration: 1032, average loss: 35.68714578716075, val loss: 33.43819778329585\n",
      "Iteration: 1033, average loss: 35.67600850091464, val loss: 33.42970602097871\n",
      "Iteration: 1034, average loss: 35.6648488804914, val loss: 33.42119696953317\n",
      "Iteration: 1035, average loss: 35.65366698808716, val loss: 33.41267216314298\n",
      "Iteration: 1036, average loss: 35.642462776478936, val loss: 33.404130804266195\n",
      "Iteration: 1037, average loss: 35.63123562716056, val loss: 33.395575963973364\n",
      "Iteration: 1038, average loss: 35.61998587645194, val loss: 33.38700158242928\n",
      "Iteration: 1039, average loss: 35.608712816008044, val loss: 33.37840980709638\n",
      "Iteration: 1040, average loss: 35.59741741217277, val loss: 33.36980358536256\n",
      "Iteration: 1041, average loss: 35.58609902225255, val loss: 33.36118008873086\n",
      "Iteration: 1042, average loss: 35.57475831197656, val loss: 33.35254082888554\n",
      "Iteration: 1043, average loss: 35.563394172180104, val loss: 33.34388312057827\n",
      "Iteration: 1044, average loss: 35.55200617785615, val loss: 33.33520543052268\n",
      "Iteration: 1045, average loss: 35.54059528843792, val loss: 33.326513009270535\n",
      "Iteration: 1046, average loss: 35.52916032795745, val loss: 33.317800761703246\n",
      "Iteration: 1047, average loss: 35.51770167880588, val loss: 33.30906462497563\n",
      "Iteration: 1048, average loss: 35.50621853819216, val loss: 33.3003155278319\n",
      "Iteration: 1049, average loss: 35.49471249096636, val loss: 33.2915492310117\n",
      "Iteration: 1050, average loss: 35.48318255581142, val loss: 33.282766102038394\n",
      "Iteration: 1051, average loss: 35.47162919113602, val loss: 33.273961923004116\n",
      "Iteration: 1052, average loss: 35.46005168398797, val loss: 33.26514326646881\n",
      "Iteration: 1053, average loss: 35.44845155586943, val loss: 33.25630519109978\n",
      "Iteration: 1054, average loss: 35.43682681535177, val loss: 33.247450704874716\n",
      "Iteration: 1055, average loss: 35.42517823067264, val loss: 33.238580835302784\n",
      "Iteration: 1056, average loss: 35.4135072450131, val loss: 33.22969536932945\n",
      "Iteration: 1057, average loss: 35.401812976109234, val loss: 33.220794330598196\n",
      "Iteration: 1058, average loss: 35.39009542396103, val loss: 33.211871008081985\n",
      "Iteration: 1059, average loss: 35.37835349783229, val loss: 33.20293728401805\n",
      "Iteration: 1060, average loss: 35.366588660484354, val loss: 33.19397929042693\n",
      "Iteration: 1061, average loss: 35.35479899765789, val loss: 33.185009283341145\n",
      "Iteration: 1062, average loss: 35.34298555516966, val loss: 33.17601715299461\n",
      "Iteration: 1063, average loss: 35.33114908973952, val loss: 33.16700930557503\n",
      "Iteration: 1064, average loss: 35.31928731162767, val loss: 33.157983641373775\n",
      "Iteration: 1065, average loss: 35.30740350340876, val loss: 33.148941022783056\n",
      "Iteration: 1066, average loss: 35.29549463474808, val loss: 33.13988593643395\n",
      "Iteration: 1067, average loss: 35.28356217416588, val loss: 33.13082120658254\n",
      "Iteration: 1068, average loss: 35.27160766850347, val loss: 33.121737836313784\n",
      "Iteration: 1069, average loss: 35.25962811967601, val loss: 33.11262295293669\n",
      "Iteration: 1070, average loss: 35.24762349082652, val loss: 33.103500101277625\n",
      "Iteration: 1071, average loss: 35.23559386603498, val loss: 33.09435865081352\n",
      "Iteration: 1072, average loss: 35.22354028420748, val loss: 33.08518772936937\n",
      "Iteration: 1073, average loss: 35.21146045790778, val loss: 33.07602188778007\n",
      "Iteration: 1074, average loss: 35.19935975443338, val loss: 33.06681989138061\n",
      "Iteration: 1075, average loss: 35.18723134372545, val loss: 33.05760539095595\n",
      "Iteration: 1076, average loss: 35.17507926162315, val loss: 33.04836705655602\n",
      "Iteration: 1077, average loss: 35.162901561617275, val loss: 33.03911062171475\n",
      "Iteration: 1078, average loss: 35.15069956364839, val loss: 33.02984354954966\n",
      "Iteration: 1079, average loss: 35.13847239466681, val loss: 33.020546724833\n",
      "Iteration: 1080, average loss: 35.12622003624405, val loss: 33.01123400895831\n",
      "Iteration: 1081, average loss: 35.113943397134975, val loss: 33.00191217914207\n",
      "Iteration: 1082, average loss: 35.10164281250774, val loss: 32.99255495884192\n",
      "Iteration: 1083, average loss: 35.08931612622911, val loss: 32.98320395739448\n",
      "Iteration: 1084, average loss: 35.076967167969485, val loss: 32.97381454530486\n",
      "Iteration: 1085, average loss: 35.06459089523352, val loss: 32.9644102776995\n",
      "Iteration: 1086, average loss: 35.05218990874175, val loss: 32.95498801963974\n",
      "Iteration: 1087, average loss: 35.039762857455564, val loss: 32.94554581062795\n",
      "Iteration: 1088, average loss: 35.02731099451222, val loss: 32.93608669858538\n",
      "Iteration: 1089, average loss: 35.014833940975905, val loss: 32.92660683936025\n",
      "Iteration: 1090, average loss: 35.00233031355816, val loss: 32.91711599317092\n",
      "Iteration: 1091, average loss: 34.98980160151127, val loss: 32.9076004482955\n",
      "Iteration: 1092, average loss: 34.97724714716851, val loss: 32.89807408833892\n",
      "Iteration: 1093, average loss: 34.9646677567763, val loss: 32.888521534489186\n",
      "Iteration: 1094, average loss: 34.952061066880894, val loss: 32.87895990344653\n",
      "Iteration: 1095, average loss: 34.93943042225308, val loss: 32.869377010505694\n",
      "Iteration: 1096, average loss: 34.92677367597386, val loss: 32.8597677373311\n",
      "Iteration: 1097, average loss: 34.914092356456074, val loss: 32.85015152851075\n",
      "Iteration: 1098, average loss: 34.901385027429335, val loss: 32.84050865492127\n",
      "Iteration: 1099, average loss: 34.888651346814804, val loss: 32.830843436638574\n",
      "Iteration: 1100, average loss: 34.875890359786396, val loss: 32.82115566584408\n",
      "Iteration: 1101, average loss: 34.863103282624394, val loss: 32.811456107736845\n",
      "Iteration: 1102, average loss: 34.85028998287404, val loss: 32.80172922640368\n",
      "Iteration: 1103, average loss: 34.83744923619256, val loss: 32.79198083812767\n",
      "Iteration: 1104, average loss: 34.82458186149597, val loss: 32.78221941941682\n",
      "Iteration: 1105, average loss: 34.81168830797868, val loss: 32.77243585096268\n",
      "Iteration: 1106, average loss: 34.798766839907366, val loss: 32.76263070659507\n",
      "Iteration: 1107, average loss: 34.78581936693422, val loss: 32.752802492629364\n",
      "Iteration: 1108, average loss: 34.772845219874725, val loss: 32.74296907606656\n",
      "Iteration: 1109, average loss: 34.75984426627412, val loss: 32.733100637907725\n",
      "Iteration: 1110, average loss: 34.746816775649066, val loss: 32.72322248700249\n",
      "Iteration: 1111, average loss: 34.73376253031302, val loss: 32.713323351899575\n",
      "Iteration: 1112, average loss: 34.72068275000162, val loss: 32.70339616262568\n",
      "Iteration: 1113, average loss: 34.7075761458724, val loss: 32.693475429342925\n",
      "Iteration: 1114, average loss: 34.69444472202356, val loss: 32.68350050542863\n",
      "Iteration: 1115, average loss: 34.68128453706198, val loss: 32.673534767612615\n",
      "Iteration: 1116, average loss: 34.66809900256171, val loss: 32.663538646950926\n",
      "Iteration: 1117, average loss: 34.6548860464695, val loss: 32.65352256550709\n",
      "Iteration: 1118, average loss: 34.64164511247534, val loss: 32.64348286689564\n",
      "Iteration: 1119, average loss: 34.62837655993476, val loss: 32.63343730217722\n",
      "Iteration: 1120, average loss: 34.615082313473096, val loss: 32.62335623330124\n",
      "Iteration: 1121, average loss: 34.60175816909127, val loss: 32.61327467139533\n",
      "Iteration: 1122, average loss: 34.588409806219275, val loss: 32.60316008114693\n",
      "Iteration: 1123, average loss: 34.57503322357141, val loss: 32.59303455134093\n",
      "Iteration: 1124, average loss: 34.5616314134736, val loss: 32.582883557742456\n",
      "Iteration: 1125, average loss: 34.54820164620588, val loss: 32.572723890519846\n",
      "Iteration: 1126, average loss: 34.534747198584, val loss: 32.56253031265361\n",
      "Iteration: 1127, average loss: 34.52126462793581, val loss: 32.55232739478709\n",
      "Iteration: 1128, average loss: 34.50775599940388, val loss: 32.54210976286786\n",
      "Iteration: 1129, average loss: 34.49421946207682, val loss: 32.53185390720708\n",
      "Iteration: 1130, average loss: 34.48065371789794, val loss: 32.521601029605165\n",
      "Iteration: 1131, average loss: 34.46706287642032, val loss: 32.511302765192326\n",
      "Iteration: 1132, average loss: 34.45344310106287, val loss: 32.50100799669116\n",
      "Iteration: 1133, average loss: 34.43979672763658, val loss: 32.49067860751554\n",
      "Iteration: 1134, average loss: 34.42612073386925, val loss: 32.48034809981665\n",
      "Iteration: 1135, average loss: 34.41241746708967, val loss: 32.46997955014579\n",
      "Iteration: 1136, average loss: 34.39868475734324, val loss: 32.459606758061696\n",
      "Iteration: 1137, average loss: 34.38492607840018, val loss: 32.44919800049803\n",
      "Iteration: 1138, average loss: 34.37113911287796, val loss: 32.43878725397296\n",
      "Iteration: 1139, average loss: 34.35732534081464, val loss: 32.42834003340578\n",
      "Iteration: 1140, average loss: 34.343482637175036, val loss: 32.41788126548353\n",
      "Iteration: 1141, average loss: 34.329612827531385, val loss: 32.40739604857161\n",
      "Iteration: 1142, average loss: 34.31571612496307, val loss: 32.396902163139316\n",
      "Iteration: 1143, average loss: 34.30179320326174, val loss: 32.38637545459836\n",
      "Iteration: 1144, average loss: 34.28784219884642, val loss: 32.37583734018301\n",
      "Iteration: 1145, average loss: 34.273862805343484, val loss: 32.36527690701298\n",
      "Iteration: 1146, average loss: 34.259855143689876, val loss: 32.35468727811626\n",
      "Iteration: 1147, average loss: 34.24581750233968, val loss: 32.34408697757368\n",
      "Iteration: 1148, average loss: 34.23175080617269, val loss: 32.333457178391555\n",
      "Iteration: 1149, average loss: 34.217655125447514, val loss: 32.32284213306373\n",
      "Iteration: 1150, average loss: 34.203533039000874, val loss: 32.31218911015702\n",
      "Iteration: 1151, average loss: 34.18938051790431, val loss: 32.301506566540766\n",
      "Iteration: 1152, average loss: 34.17519928522156, val loss: 32.2908226923545\n",
      "Iteration: 1153, average loss: 34.16099046278691, val loss: 32.2801144206125\n",
      "Iteration: 1154, average loss: 34.14675223654595, val loss: 32.26937531571625\n",
      "Iteration: 1155, average loss: 34.132485658074344, val loss: 32.25862627636382\n",
      "Iteration: 1156, average loss: 34.118191091334765, val loss: 32.247862114263754\n",
      "Iteration: 1157, average loss: 34.103868636532106, val loss: 32.23706502018398\n",
      "Iteration: 1158, average loss: 34.08951682284258, val loss: 32.22624961442925\n",
      "Iteration: 1159, average loss: 34.07513581842616, val loss: 32.21541849613132\n",
      "Iteration: 1160, average loss: 34.060726535493046, val loss: 32.20455617043601\n",
      "Iteration: 1161, average loss: 34.04628647237584, val loss: 32.19367079501812\n",
      "Iteration: 1162, average loss: 34.031815785716695, val loss: 32.18277549289864\n",
      "Iteration: 1163, average loss: 34.01731444211398, val loss: 32.171846073930425\n",
      "Iteration: 1164, average loss: 34.00278264312929, val loss: 32.16090054755092\n",
      "Iteration: 1165, average loss: 33.98822150598978, val loss: 32.149932334433515\n",
      "Iteration: 1166, average loss: 33.97363112744502, val loss: 32.138940482062786\n",
      "Iteration: 1167, average loss: 33.959010998408, val loss: 32.12793322999498\n",
      "Iteration: 1168, average loss: 33.944361344627715, val loss: 32.11690038473588\n",
      "Iteration: 1169, average loss: 33.92968338929513, val loss: 32.10585119830269\n",
      "Iteration: 1170, average loss: 33.91497919524925, val loss: 32.094781543535035\n",
      "Iteration: 1171, average loss: 33.90024661672288, val loss: 32.08368844768812\n",
      "Iteration: 1172, average loss: 33.8854843913645, val loss: 32.07257625385982\n",
      "Iteration: 1173, average loss: 33.87069269309297, val loss: 32.061440667825295\n",
      "Iteration: 1174, average loss: 33.85587028604775, val loss: 32.05029245948067\n",
      "Iteration: 1175, average loss: 33.841018343893225, val loss: 32.03910987917922\n",
      "Iteration: 1176, average loss: 33.82613468285344, val loss: 32.02791856220356\n",
      "Iteration: 1177, average loss: 33.811222736386284, val loss: 32.01670554155913\n",
      "Iteration: 1178, average loss: 33.79628166829906, val loss: 32.00547279223324\n",
      "Iteration: 1179, average loss: 33.78131212128533, val loss: 31.994219251250353\n",
      "Iteration: 1180, average loss: 33.766313990533064, val loss: 31.982951671582704\n",
      "Iteration: 1181, average loss: 33.75128803967278, val loss: 31.971667646782084\n",
      "Iteration: 1182, average loss: 33.73623179813514, val loss: 31.96034600064097\n",
      "Iteration: 1183, average loss: 33.72114600536328, val loss: 31.94900555669115\n",
      "Iteration: 1184, average loss: 33.70602961323687, val loss: 31.93765075600923\n",
      "Iteration: 1185, average loss: 33.69088533765452, val loss: 31.926275722490676\n",
      "Iteration: 1186, average loss: 33.67571123901772, val loss: 31.91487623813979\n",
      "Iteration: 1187, average loss: 33.660507501611384, val loss: 31.903450724298402\n",
      "Iteration: 1188, average loss: 33.645271846061746, val loss: 31.89199738263676\n",
      "Iteration: 1189, average loss: 33.6300046812509, val loss: 31.880521780017723\n",
      "Iteration: 1190, average loss: 33.61470590467038, val loss: 31.869035346772744\n",
      "Iteration: 1191, average loss: 33.59937626843291, val loss: 31.857515871444207\n",
      "Iteration: 1192, average loss: 33.58401521853203, val loss: 31.84598619657111\n",
      "Iteration: 1193, average loss: 33.56862673667318, val loss: 31.83443433932614\n",
      "Iteration: 1194, average loss: 33.55320830391225, val loss: 31.822855783294138\n",
      "Iteration: 1195, average loss: 33.53776025772095, val loss: 31.811261746053\n",
      "Iteration: 1196, average loss: 33.522283673862326, val loss: 31.79964845790684\n",
      "Iteration: 1197, average loss: 33.50677862950569, val loss: 31.788012460381378\n",
      "Iteration: 1198, average loss: 33.49124410762879, val loss: 31.776363085296236\n",
      "Iteration: 1199, average loss: 33.47567968667993, val loss: 31.764698060412087\n",
      "Iteration: 1200, average loss: 33.46008544152485, val loss: 31.753005399102456\n",
      "Iteration: 1201, average loss: 33.44445953276998, val loss: 31.741293052701185\n",
      "Iteration: 1202, average loss: 33.42880276435815, val loss: 31.729570802983467\n",
      "Iteration: 1203, average loss: 33.4131162903735, val loss: 31.717814947044342\n",
      "Iteration: 1204, average loss: 33.39739876783988, val loss: 31.70605309707191\n",
      "Iteration: 1205, average loss: 33.38164986619627, val loss: 31.694251571517878\n",
      "Iteration: 1206, average loss: 33.36587028918059, val loss: 31.682443595010266\n",
      "Iteration: 1207, average loss: 33.35006246359452, val loss: 31.670616580927668\n",
      "Iteration: 1208, average loss: 33.3342247504543, val loss: 31.65875968283342\n",
      "Iteration: 1209, average loss: 33.318356397647214, val loss: 31.64689865773919\n",
      "Iteration: 1210, average loss: 33.30245961774374, val loss: 31.63499275681402\n",
      "Iteration: 1211, average loss: 33.28653009156674, val loss: 31.623086183462412\n",
      "Iteration: 1212, average loss: 33.2705716015636, val loss: 31.611141863502866\n",
      "Iteration: 1213, average loss: 33.25458115886375, val loss: 31.599193057183896\n",
      "Iteration: 1214, average loss: 33.23856093572534, val loss: 31.587203226307928\n",
      "Iteration: 1215, average loss: 33.22250640795426, val loss: 31.575203320352774\n",
      "Iteration: 1216, average loss: 33.20642247868045, val loss: 31.563170040763072\n",
      "Iteration: 1217, average loss: 33.190307769222535, val loss: 31.5511413181452\n",
      "Iteration: 1218, average loss: 33.17416525923687, val loss: 31.53906622235948\n",
      "Iteration: 1219, average loss: 33.157991918388774, val loss: 31.526993851036185\n",
      "Iteration: 1220, average loss: 33.141791405885115, val loss: 31.514876566097712\n",
      "Iteration: 1221, average loss: 33.125557394995205, val loss: 31.502752403958652\n",
      "Iteration: 1222, average loss: 33.10929291490196, val loss: 31.49060161828599\n",
      "Iteration: 1223, average loss: 33.09299718354635, val loss: 31.47843977597365\n",
      "Iteration: 1224, average loss: 33.07667240773998, val loss: 31.466244771568448\n",
      "Iteration: 1225, average loss: 33.06031432474293, val loss: 31.45404232653799\n",
      "Iteration: 1226, average loss: 33.04392648088759, val loss: 31.44180857188914\n",
      "Iteration: 1227, average loss: 33.02750609577566, val loss: 31.42955287572419\n",
      "Iteration: 1228, average loss: 33.01105483027472, val loss: 31.417288378570152\n",
      "Iteration: 1229, average loss: 32.994571666210746, val loss: 31.404987317823622\n",
      "Iteration: 1230, average loss: 32.97805554394561, val loss: 31.392686796746155\n",
      "Iteration: 1231, average loss: 32.96150965851862, val loss: 31.38034323586429\n",
      "Iteration: 1232, average loss: 32.94492941893241, val loss: 31.367984182717617\n",
      "Iteration: 1233, average loss: 32.92831680740135, val loss: 31.35561402787972\n",
      "Iteration: 1234, average loss: 32.911674075656464, val loss: 31.34320622916467\n",
      "Iteration: 1235, average loss: 32.894995560392665, val loss: 31.33077671905077\n",
      "Iteration: 1236, average loss: 32.878285475975076, val loss: 31.318327518002206\n",
      "Iteration: 1237, average loss: 32.8615406319715, val loss: 31.30583946301985\n",
      "Iteration: 1238, average loss: 32.844760967337564, val loss: 31.293340543456697\n",
      "Iteration: 1239, average loss: 32.827949404140604, val loss: 31.280814348351488\n",
      "Iteration: 1240, average loss: 32.81110643303913, val loss: 31.26826617540044\n",
      "Iteration: 1241, average loss: 32.7942316946776, val loss: 31.255694992385816\n",
      "Iteration: 1242, average loss: 32.777327407385414, val loss: 31.243108432271505\n",
      "Iteration: 1243, average loss: 32.760391523296704, val loss: 31.230496320032188\n",
      "Iteration: 1244, average loss: 32.743423665779225, val loss: 31.217849210154885\n",
      "Iteration: 1245, average loss: 32.726421858377506, val loss: 31.20521476796632\n",
      "Iteration: 1246, average loss: 32.70939038341172, val loss: 31.19253050546166\n",
      "Iteration: 1247, average loss: 32.69232698569551, val loss: 31.17984985496521\n",
      "Iteration: 1248, average loss: 32.67523525532893, val loss: 31.167158268381446\n",
      "Iteration: 1249, average loss: 32.65811542381987, val loss: 31.15443023940129\n",
      "Iteration: 1250, average loss: 32.6409666330918, val loss: 31.141714986247546\n",
      "Iteration: 1251, average loss: 32.623791717676724, val loss: 31.128942247779413\n",
      "Iteration: 1252, average loss: 32.606584090541524, val loss: 31.116184425447603\n",
      "Iteration: 1253, average loss: 32.58935072686937, val loss: 31.103384091025827\n",
      "Iteration: 1254, average loss: 32.57208541395583, val loss: 31.090563957173185\n",
      "Iteration: 1255, average loss: 32.55478916421605, val loss: 31.077724386623878\n",
      "Iteration: 1256, average loss: 32.53746283572653, val loss: 31.06487266958755\n",
      "Iteration: 1257, average loss: 32.52010561878554, val loss: 31.051994464387878\n",
      "Iteration: 1258, average loss: 32.50271631323773, val loss: 31.039080732781713\n",
      "Iteration: 1259, average loss: 32.485292101827795, val loss: 31.026182334388643\n",
      "Iteration: 1260, average loss: 32.467837766748694, val loss: 31.013196505834173\n",
      "Iteration: 1261, average loss: 32.45034573504314, val loss: 31.00026321850416\n",
      "Iteration: 1262, average loss: 32.43282785277436, val loss: 30.987237600278227\n",
      "Iteration: 1263, average loss: 32.41527313195565, val loss: 30.974254419018543\n",
      "Iteration: 1264, average loss: 32.39769459001108, val loss: 30.961209321962418\n",
      "Iteration: 1265, average loss: 32.38008328566804, val loss: 30.94818433776831\n",
      "Iteration: 1266, average loss: 32.362442583277605, val loss: 30.935082321269114\n",
      "Iteration: 1267, average loss: 32.34476690476643, val loss: 30.922016359805088\n",
      "Iteration: 1268, average loss: 32.32706186967196, val loss: 30.90889196196826\n",
      "Iteration: 1269, average loss: 32.30932348822626, val loss: 30.895768943639872\n",
      "Iteration: 1270, average loss: 32.29155397760695, val loss: 30.882608609295335\n",
      "Iteration: 1271, average loss: 32.27375247513039, val loss: 30.869442613785843\n",
      "Iteration: 1272, average loss: 32.25591990567636, val loss: 30.856233968131164\n",
      "Iteration: 1273, average loss: 32.23805419488806, val loss: 30.84303064325703\n",
      "Iteration: 1274, average loss: 32.220156001583966, val loss: 30.829775358423802\n",
      "Iteration: 1275, average loss: 32.20222250966058, val loss: 30.8165156942613\n",
      "Iteration: 1276, average loss: 32.18425891134474, val loss: 30.803240134503376\n",
      "Iteration: 1277, average loss: 32.16626595068669, val loss: 30.789932161934992\n",
      "Iteration: 1278, average loss: 32.148240677976375, val loss: 30.77660701641591\n",
      "Iteration: 1279, average loss: 32.13018216027154, val loss: 30.76324499282572\n",
      "Iteration: 1280, average loss: 32.11208977445888, val loss: 30.749897474713986\n",
      "Iteration: 1281, average loss: 32.09396945911905, val loss: 30.736481904202538\n",
      "Iteration: 1282, average loss: 32.075810296523976, val loss: 30.723073146997955\n",
      "Iteration: 1283, average loss: 32.057619197357106, val loss: 30.709621117282296\n",
      "Iteration: 1284, average loss: 32.03939578498619, val loss: 30.69614372209758\n",
      "Iteration: 1285, average loss: 32.02114193451001, val loss: 30.682688191081382\n",
      "Iteration: 1286, average loss: 32.00286141916174, val loss: 30.669169677065383\n",
      "Iteration: 1287, average loss: 31.984545821728915, val loss: 30.655664650774145\n",
      "Iteration: 1288, average loss: 31.96619995550257, val loss: 30.642116375324633\n",
      "Iteration: 1289, average loss: 31.947821280806536, val loss: 30.62856360166503\n",
      "Iteration: 1290, average loss: 31.92941351904385, val loss: 30.614977168467846\n",
      "Iteration: 1291, average loss: 31.91097223125218, val loss: 30.601407701348087\n",
      "Iteration: 1292, average loss: 31.89250255667645, val loss: 30.58778156545484\n",
      "Iteration: 1293, average loss: 31.873999926203116, val loss: 30.574160929973555\n",
      "Iteration: 1294, average loss: 31.855465209426512, val loss: 30.560505043130753\n",
      "Iteration: 1295, average loss: 31.836900028053687, val loss: 30.546838442153238\n",
      "Iteration: 1296, average loss: 31.818301954131197, val loss: 30.533134390246136\n",
      "Iteration: 1297, average loss: 31.799672782133168, val loss: 30.519447483338205\n",
      "Iteration: 1298, average loss: 31.78101597085667, val loss: 30.505702117047214\n",
      "Iteration: 1299, average loss: 31.762324049852896, val loss: 30.491974299526582\n",
      "Iteration: 1300, average loss: 31.743603770859576, val loss: 30.47818447366608\n",
      "Iteration: 1301, average loss: 31.724846577298813, val loss: 30.464422300301568\n",
      "Iteration: 1302, average loss: 31.70606179398615, val loss: 30.450571235359945\n",
      "Iteration: 1303, average loss: 31.687237767205723, val loss: 30.436780700201872\n",
      "Iteration: 1304, average loss: 31.6683856461935, val loss: 30.42288795728058\n",
      "Iteration: 1305, average loss: 31.649494211454897, val loss: 30.409037444748527\n",
      "Iteration: 1306, average loss: 31.63057439453936, val loss: 30.39511377474584\n",
      "Iteration: 1307, average loss: 31.611619501298176, val loss: 30.381214349012673\n",
      "Iteration: 1308, average loss: 31.592633882006584, val loss: 30.367256862531953\n",
      "Iteration: 1309, average loss: 31.573611851475665, val loss: 30.353310740607093\n",
      "Iteration: 1310, average loss: 31.554558798886728, val loss: 30.339308244905954\n",
      "Iteration: 1311, average loss: 31.53547059856175, val loss: 30.32532157963316\n",
      "Iteration: 1312, average loss: 31.516349220045523, val loss: 30.311305448010444\n",
      "Iteration: 1313, average loss: 31.49719538435268, val loss: 30.29726824015459\n",
      "Iteration: 1314, average loss: 31.47800855129813, val loss: 30.28320818191051\n",
      "Iteration: 1315, average loss: 31.45878817263433, val loss: 30.26913249209821\n",
      "Iteration: 1316, average loss: 31.439537949032253, val loss: 30.255063750220053\n",
      "Iteration: 1317, average loss: 31.420257427842145, val loss: 30.24095804531263\n",
      "Iteration: 1318, average loss: 31.40094646163609, val loss: 30.22683206237939\n",
      "Iteration: 1319, average loss: 31.381606356533254, val loss: 30.21271865973315\n",
      "Iteration: 1320, average loss: 31.36224062776796, val loss: 30.198527736617187\n",
      "Iteration: 1321, average loss: 31.342840740646142, val loss: 30.184383592016577\n",
      "Iteration: 1322, average loss: 31.323415011023553, val loss: 30.170145939570755\n",
      "Iteration: 1323, average loss: 31.303954789028076, val loss: 30.155938789302574\n",
      "Iteration: 1324, average loss: 31.28446614223978, val loss: 30.14166917033807\n",
      "Iteration: 1325, average loss: 31.26494344536233, val loss: 30.127430114979642\n",
      "Iteration: 1326, average loss: 31.245393507722497, val loss: 30.113104031803665\n",
      "Iteration: 1327, average loss: 31.225811271851764, val loss: 30.098844135987754\n",
      "Iteration: 1328, average loss: 31.2062044189748, val loss: 30.08445355241426\n",
      "Iteration: 1329, average loss: 31.186557783596758, val loss: 30.070165318531714\n",
      "Iteration: 1330, average loss: 31.166887169298917, val loss: 30.05574524823076\n",
      "Iteration: 1331, average loss: 31.147179494157506, val loss: 30.04141886099196\n",
      "Iteration: 1332, average loss: 31.12744935583953, val loss: 30.02694237484136\n",
      "Iteration: 1333, average loss: 31.10767638165018, val loss: 30.0125840620377\n",
      "Iteration: 1334, average loss: 31.087882627035686, val loss: 29.998104445234002\n",
      "Iteration: 1335, average loss: 31.06804809938882, val loss: 29.983655953282142\n",
      "Iteration: 1336, average loss: 31.048186015391696, val loss: 29.96915186250369\n",
      "Iteration: 1337, average loss: 31.028288886167, val loss: 29.954645839682314\n",
      "Iteration: 1338, average loss: 31.008362946302995, val loss: 29.94012285948465\n",
      "Iteration: 1339, average loss: 30.988406834395036, val loss: 29.92557330644617\n",
      "Iteration: 1340, average loss: 30.968419533420875, val loss: 29.910998002623842\n",
      "Iteration: 1341, average loss: 30.948399730350662, val loss: 29.896412408660257\n",
      "Iteration: 1342, average loss: 30.928346878088615, val loss: 29.88179515638336\n",
      "Iteration: 1343, average loss: 30.908260890251196, val loss: 29.86716479223487\n",
      "Iteration: 1344, average loss: 30.888140960592004, val loss: 29.852485703363442\n",
      "Iteration: 1345, average loss: 30.867984714139485, val loss: 29.83782157223639\n",
      "Iteration: 1346, average loss: 30.84779997724266, val loss: 29.823118147543504\n",
      "Iteration: 1347, average loss: 30.827586899632994, val loss: 29.808412023489815\n",
      "Iteration: 1348, average loss: 30.807342649082056, val loss: 29.79367782757775\n",
      "Iteration: 1349, average loss: 30.787065843453153, val loss: 29.778926476362948\n",
      "Iteration: 1350, average loss: 30.766758081417727, val loss: 29.76416118237621\n",
      "Iteration: 1351, average loss: 30.746417927857184, val loss: 29.749355323931418\n",
      "Iteration: 1352, average loss: 30.726037198218748, val loss: 29.734532939808417\n",
      "Iteration: 1353, average loss: 30.705615921296936, val loss: 29.719674667513317\n",
      "Iteration: 1354, average loss: 30.685158210100184, val loss: 29.704821294977847\n",
      "Iteration: 1355, average loss: 30.664671742397807, val loss: 29.689928595778753\n",
      "Iteration: 1356, average loss: 30.644157254177593, val loss: 29.675055177943886\n",
      "Iteration: 1357, average loss: 30.623615066786318, val loss: 29.660119983329878\n",
      "Iteration: 1358, average loss: 30.603037906729657, val loss: 29.64519339841899\n",
      "Iteration: 1359, average loss: 30.582434924904273, val loss: 29.630246509988933\n",
      "Iteration: 1360, average loss: 30.561810713459327, val loss: 29.61529487101991\n",
      "Iteration: 1361, average loss: 30.541164370550625, val loss: 29.600320554909246\n",
      "Iteration: 1362, average loss: 30.52049267349612, val loss: 29.58536462468898\n",
      "Iteration: 1363, average loss: 30.49979315287825, val loss: 29.570322344797596\n",
      "Iteration: 1364, average loss: 30.479060687880587, val loss: 29.55533894386513\n",
      "Iteration: 1365, average loss: 30.458304205954363, val loss: 29.540268485787664\n",
      "Iteration: 1366, average loss: 30.437516689300537, val loss: 29.52521975191798\n",
      "Iteration: 1367, average loss: 30.41670380813488, val loss: 29.510157146910444\n",
      "Iteration: 1368, average loss: 30.395859623876746, val loss: 29.495033520711484\n",
      "Iteration: 1369, average loss: 30.37498304118281, val loss: 29.479980826553117\n",
      "Iteration: 1370, average loss: 30.35408095461159, val loss: 29.46479988515228\n",
      "Iteration: 1371, average loss: 30.3331424948098, val loss: 29.449726178669483\n",
      "Iteration: 1372, average loss: 30.31218193579411, val loss: 29.434523462560286\n",
      "Iteration: 1373, average loss: 30.291181985882744, val loss: 29.419397687983952\n",
      "Iteration: 1374, average loss: 30.270157448911437, val loss: 29.404164210452933\n",
      "Iteration: 1375, average loss: 30.24909750159812, val loss: 29.389003191187225\n",
      "Iteration: 1376, average loss: 30.228015997559552, val loss: 29.373741814962443\n",
      "Iteration: 1377, average loss: 30.20690167007815, val loss: 29.358563606829733\n",
      "Iteration: 1378, average loss: 30.185768613492808, val loss: 29.34325448928194\n",
      "Iteration: 1379, average loss: 30.164602056217653, val loss: 29.328096536226603\n",
      "Iteration: 1380, average loss: 30.143418996230416, val loss: 29.312680390282758\n",
      "Iteration: 1381, average loss: 30.122192234232806, val loss: 29.297592498911552\n",
      "Iteration: 1382, average loss: 30.100961560788363, val loss: 29.282045151618128\n",
      "Iteration: 1383, average loss: 30.079672710907055, val loss: 29.267038353165127\n",
      "Iteration: 1384, average loss: 30.058393377036865, val loss: 29.25133063559022\n",
      "Iteration: 1385, average loss: 30.037041475231522, val loss: 29.236427345572615\n",
      "Iteration: 1386, average loss: 30.015719637202757, val loss: 29.22051268190318\n",
      "Iteration: 1387, average loss: 29.994306498679563, val loss: 29.205807144722144\n",
      "Iteration: 1388, average loss: 29.972947558343122, val loss: 29.18961562512188\n",
      "Iteration: 1389, average loss: 29.95146788030431, val loss: 29.175165677121942\n",
      "Iteration: 1390, average loss: 29.930077418037083, val loss: 29.158612047576515\n",
      "Iteration: 1391, average loss: 29.908522005818316, val loss: 29.144551409214518\n",
      "Iteration: 1392, average loss: 29.88710281699176, val loss: 29.127443808850586\n",
      "Iteration: 1393, average loss: 29.865459685164375, val loss: 29.113974040277306\n",
      "Iteration: 1394, average loss: 29.844024653596, val loss: 29.096102997133364\n",
      "Iteration: 1395, average loss: 29.82227919412696, val loss: 29.083466538289507\n",
      "Iteration: 1396, average loss: 29.80084595127382, val loss: 29.064538149520967\n",
      "Iteration: 1397, average loss: 29.778979651593932, val loss: 29.053133362357745\n",
      "Iteration: 1398, average loss: 29.757575616744404, val loss: 29.032637132095054\n",
      "Iteration: 1399, average loss: 29.735559996775383, val loss: 29.023095860121128\n",
      "Iteration: 1400, average loss: 29.714226210174928, val loss: 29.000280676013084\n",
      "Iteration: 1401, average loss: 29.692014691910305, val loss: 28.993554252447883\n",
      "Iteration: 1402, average loss: 29.670818520052997, val loss: 28.967232547732532\n",
      "Iteration: 1403, average loss: 29.648348934984437, val loss: 28.964868060966904\n",
      "Iteration: 1404, average loss: 29.6274106560122, val loss: 28.933131524865274\n",
      "Iteration: 1405, average loss: 29.604593447441065, val loss: 28.93772888731259\n",
      "Iteration: 1406, average loss: 29.584128736873755, val loss: 28.897346750886925\n",
      "Iteration: 1407, average loss: 29.560837070723085, val loss: 28.913286788656517\n",
      "Iteration: 1408, average loss: 29.541256092596747, val loss: 28.859059025704248\n",
      "Iteration: 1409, average loss: 29.51737215553505, val loss: 28.893693192830796\n",
      "Iteration: 1410, average loss: 29.499479896204484, val loss: 28.817263184418334\n",
      "Iteration: 1411, average loss: 29.475035710035314, val loss: 28.883503295342972\n",
      "Iteration: 1412, average loss: 29.460626143764184, val loss: 28.771101495037406\n",
      "Iteration: 1413, average loss: 29.43632928300019, val loss: 28.892201950412062\n",
      "Iteration: 1414, average loss: 29.429579859194547, val loss: 28.72236791490949\n",
      "Iteration: 1415, average loss: 29.40858514758124, val loss: 28.942122052817364\n",
      "Iteration: 1416, average loss: 29.42016873843428, val loss: 28.68337080293947\n",
      "1416 iterations have been performed. The average loss is 29.40858514758124\n"
     ]
    }
   ],
   "source": [
    "####################TODO####################\n",
    "# TODO: Make accurate predictions on the validation nodes.\n",
    "#\n",
    "# NOTE: See task description before Section 1.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from copy import deepcopy\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.layer1 = nn.Linear(7, 512)\n",
    "        self.layer2 = nn.Linear(512, 512)\n",
    "        self.layer3 = nn.Linear(512, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)\n",
    "def val_nodes_avg_error(graph_FMI, model):\n",
    "    # Copy the nodes to a new graph.\n",
    "    graph = graph_FMI.copy()\n",
    "    \n",
    "    # Get validation nodes.\n",
    "    _, val_nodes = get_train_val_nodes(graph)\n",
    "    \n",
    "    # Create storage for the validation errors.\n",
    "    val_errors = np.zeros(len(val_nodes))\n",
    "    \n",
    "    for i, val_node in enumerate(val_nodes):\n",
    "        # Calculate the errors of validation nodes\n",
    "        X_val_node = graph.nodes[val_node]['X']\n",
    "        y_val_node = graph.nodes[val_node]['y']\n",
    "        w_val_node = graph.nodes[val_node]['weights']\n",
    "        with torch.no_grad():\n",
    "            val_errors[i] = mean_squared_error(y_val_node, model(torch.tensor(X_val_node, dtype=torch.float32)).flatten().numpy())\n",
    "        \n",
    "    return np.mean(val_errors)\n",
    "\n",
    "# Initialize the model in each node\n",
    "train_nodes, val_nodes = get_train_val_nodes(G_FMI)\n",
    "\n",
    "# Define hyperparameters\n",
    "alpha = 0.5\n",
    "l_rate = 0.001\n",
    "\n",
    "# Define initial values and storage\n",
    "prev_loss_avg = 1e10\n",
    "curr_loss_avg = 1e9\n",
    "tol = 0.01\n",
    "n_iterations = 0\n",
    "\n",
    "global_model = NeuralNetwork()\n",
    "for node in train_nodes:\n",
    "    G_FMI.nodes[node]['model'] = deepcopy(global_model)\n",
    "    G_FMI.nodes[node]['optimizer'] = optim.SGD(G_FMI.nodes[node]['model'].parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "total_data = sum(map(lambda item: G_FMI.nodes[item][\"samplesize\"], train_nodes))\n",
    "\n",
    "# Iterate while the average loss over all nodes is decreasing\n",
    "while curr_loss_avg < prev_loss_avg:\n",
    "    \n",
    "    n_iterations += 1\n",
    "    losses = np.zeros(n_stations)\n",
    "\n",
    "    \n",
    "    # Iterate over all nodes\n",
    "    for current_node in train_nodes:\n",
    "        model = G_FMI.nodes[current_node]['model']\n",
    "        optimizer = G_FMI.nodes[current_node]['optimizer']\n",
    "        \n",
    "        # Get training data.\n",
    "        X_train = torch.tensor(G_FMI.nodes[current_node]['X'], dtype=torch.float32)\n",
    "        y_train = torch.tensor(G_FMI.nodes[current_node]['y'], dtype=torch.float32)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(X_train)\n",
    "        loss = torch.nn.functional.mse_loss(outputs, y_train)\n",
    "        losses[current_node] = loss.item()\n",
    "        \n",
    "        # Compute the gradients\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    global_state_dict = OrderedDict()\n",
    "    \n",
    "    for node in train_nodes:\n",
    "        model_node = G_FMI.nodes[node][\"model\"]\n",
    "        node_size = G_FMI.nodes[node][\"samplesize\"]\n",
    "        model_node_state_dict = model_node.state_dict()\n",
    "\n",
    "        for name in model_node_state_dict:\n",
    "            if name in global_state_dict:\n",
    "                global_state_dict[name] += node_size / total_data * model_node_state_dict[name]\n",
    "            else:\n",
    "                global_state_dict[name] = node_size / total_data * model_node_state_dict[name]\n",
    "        \n",
    "    global_model.load_state_dict(global_state_dict)\n",
    "    \n",
    "    for node in train_nodes:\n",
    "        G_FMI.nodes[node][\"model\"].load_state_dict(global_state_dict)\n",
    "        \n",
    "    prev_loss_avg = curr_loss_avg\n",
    "    curr_loss_avg = np.mean(losses)\n",
    "    val_loss = val_nodes_avg_error(G_FMI, model)\n",
    "    print(f\"Iteration: {n_iterations}, average loss: {curr_loss_avg}, val loss: {val_loss}\")\n",
    "    \n",
    "print(f\"{n_iterations} iterations have been performed. The average loss is {prev_loss_avg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bc8dc0e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.68337080293947"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def val_nodes_avg_error(graph_FMI, model):\n",
    "    # Copy the nodes to a new graph.\n",
    "    graph = graph_FMI.copy()\n",
    "    \n",
    "    # Get validation nodes.\n",
    "    _, val_nodes = get_train_val_nodes(graph)\n",
    "    \n",
    "    # Create storage for the validation errors.\n",
    "    val_errors = np.zeros(len(val_nodes))\n",
    "    \n",
    "    for i, val_node in enumerate(val_nodes):\n",
    "        # Calculate the errors of validation nodes\n",
    "        X_val_node = graph.nodes[val_node]['X']\n",
    "        y_val_node = graph.nodes[val_node]['y']\n",
    "        w_val_node = graph.nodes[val_node]['weights']\n",
    "        with torch.no_grad():\n",
    "            val_errors[i] = mean_squared_error(y_val_node, model(torch.tensor(X_val_node, dtype=torch.float32)).flatten().numpy())\n",
    "        \n",
    "    return np.mean(val_errors)\n",
    "\n",
    "val_nodes_avg_error(G_FMI, global_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
